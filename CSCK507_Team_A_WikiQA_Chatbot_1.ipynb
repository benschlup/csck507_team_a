{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyP7mdF4HSrTpL0GNUKkXyvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_WikiQA_Chatbot_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 1: LSTM without Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "\n",
        "Important note: \n",
        "The dataset includes incorrect answers, labelled accordingly. Learning from these can be switches on/off (see below).\n",
        "\n",
        "In a real setting, it would be sensible to add a concept called \"answer triggering\" and exclude learning from incorrect answers. Answer triggering  first assesses a question to qualify if the model may deliver a sensible answer - otherwise let the person know that the bot does not know. Ref: https://ieeexplore.ieee.org/document/8079800\n",
        "\n",
        "In this notebook, the default is set to learn from invalid answers. This leads to more data for learning and thus a greater awareness of how sentences are constructed. And sometimes in funny conversations like with a poorly hearing dialogue partner, who provides 'perfectly valid answers - but to a different question'."
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For future study, i.e. to be mentioned in report\n",
        "* Dropout on input, embedding layers or other places\n",
        "* Check if lemmatizing on question side improves performance\n",
        "* Check if word embedding (e.g. using Word2Vec or GloVe) on question (i.e. input side) improves performance (beware of out-of-vocab)\n",
        "* Use of token dropout for data augmentation: https://aclanthology.org/2020.coling-main.379.pdf\n",
        "\n",
        "For review, and potential reference:\n",
        "* Khin, N.N., Soe, K.M., 2020. Question Answering based University Chatbot using Sequence to Sequence Model, doi:10.1109/o-cocosda50338.2020.9295021"
      ],
      "metadata": {
        "id": "AHuZjEDChQ_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Configuration and framework"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = None  # set this to None if no limit on question length\n",
        "max_answer_tokens = None    # set this to None if no limit on answer length\n",
        "\n",
        "# Model parameters\n",
        "lstm_units = 150\n",
        "embedding_units = 200\n",
        "encoder_lstm_dropout = 0.2\n",
        "encoder_lstm_recurrent_dropout = 0.2\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 50\n",
        "number_of_epochs = 100"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab482df-b287-4272-9797-60567876f25e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 15 22:06:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data acquisition and loading"
      ],
      "metadata": {
        "id": "Zd9YuELT4861"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "cf5bf84b-8520-41b7-e193-65654c7ed76f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7f6d4ce805d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Dataset preparation (pre-processing, transformation)\n",
        "Note that no cleansing as such is required, as prior analysis has shown."
      ],
      "metadata": {
        "id": "ijtnhP1p5EaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "6f3c7e37-1bab-4f5c-e142-9f9755bfbd7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers and count number of tokens\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower().strip() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower().strip()+' _STOP_' for s in df['Sentence']]\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop sentences which are too long\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    if max_question_tokens is not None:\n",
        "        df.drop(df[df['question_tokens']>max_question_tokens].index, inplace=True)\n",
        "    if max_answer_tokens is not None:\n",
        "        df.drop(df[df['answer_tokens']>max_answer_tokens+2].index, inplace=True)    "
      ],
      "metadata": {
        "id": "kSV824B9dt6K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    # Tokenize\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "\n",
        "    # Optionally remove sentences with out-of-vocabulary tokens\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "0da9c2ab-6d2c-417d-fc9a-cc71fdd098ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3cebcc-6ba0-4aff-9f38-205369c28710",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2181\n",
            "Net validation dataset size: 108\n",
            "Net test dataset size: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data for training and validation by aligning lengths (i.e. padding)\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {train_decoder_output_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9CFhVHscxG5",
        "outputId": "ba205e14-4900-420f-d7bd-377bdc70a8d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2181, 21)\n",
            "Decoder input data shape: (2181, 52)\n",
            "Decoder output data shape: (2181, 52, 6001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Modelling and training"
      ],
      "metadata": {
        "id": "-rPOfWDC5ikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "# Input layer for encoder\n",
        "enc_inputs = Input(shape=(None,), name='Encoder_Input')\n",
        "\n",
        "# Embedding layer for encoder\n",
        "enc_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, \n",
        "                          name='Encoder_Embedding')(enc_inputs)\n",
        "\n",
        "# LSTM layer for encoder\n",
        "_, state_h, state_c = LSTM(lstm_units, return_state=True, \n",
        "                           dropout=encoder_lstm_dropout,\n",
        "                           recurrent_dropout=encoder_lstm_recurrent_dropout,\n",
        "                           name='Encoder_LSTM')(enc_embedding)\n",
        "\n",
        "# Combine states from encoder LSTM layer\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "# Input layer for decoder\n",
        "dec_inputs = Input(shape=(None,), name='Decoder_Input')\n",
        "\n",
        "# Embedding layer for decoder\n",
        "dec_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, name='Decoder_Embedding')(dec_inputs)\n",
        "\n",
        "# LSTM layer for decoder\n",
        "dec_lstm = LSTM(lstm_units, return_state=True, return_sequences=True, name='Decoder_LSTM')\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "\n",
        "# Dense layer for decoder\n",
        "dec_dense = Dense(vocab_size, activation=softmax, name='Decoder_Dense')\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "# Summarised printout\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "58ab264c-b487-434d-cd4b-722b5f7cfb0a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer Encoder_LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Encoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Encoder_LSTM (LSTM)            [(None, 150),        210600      ['Encoder_Embedding[0][0]']      \n",
            "                                 (None, 150),                                                     \n",
            "                                 (None, 150)]                                                     \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 150),  210600      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 150),                     'Encoder_LSTM[0][1]',           \n",
            "                                 (None, 150)]                     'Encoder_LSTM[0][2]']           \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   906151      ['Decoder_LSTM[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,727,751\n",
            "Trainable params: 3,727,751\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=batch_size, epochs=number_of_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "8bfeff66-79a6-4ec8-8ae4-5e1d5aa4c2f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "44/44 [==============================] - 12s 135ms/step - loss: 2.2194 - val_loss: 2.0401\n",
            "Epoch 2/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 2.0200 - val_loss: 1.9818\n",
            "Epoch 3/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.9542 - val_loss: 1.9446\n",
            "Epoch 4/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.9048 - val_loss: 1.9134\n",
            "Epoch 5/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.8599 - val_loss: 1.8858\n",
            "Epoch 6/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.8182 - val_loss: 1.8596\n",
            "Epoch 7/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.7818 - val_loss: 1.8378\n",
            "Epoch 8/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.7495 - val_loss: 1.8245\n",
            "Epoch 9/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.7203 - val_loss: 1.8136\n",
            "Epoch 10/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.6932 - val_loss: 1.8013\n",
            "Epoch 11/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.6668 - val_loss: 1.7940\n",
            "Epoch 12/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.6407 - val_loss: 1.7868\n",
            "Epoch 13/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.6146 - val_loss: 1.7753\n",
            "Epoch 14/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.5881 - val_loss: 1.7667\n",
            "Epoch 15/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.5623 - val_loss: 1.7612\n",
            "Epoch 16/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.5364 - val_loss: 1.7519\n",
            "Epoch 17/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.5107 - val_loss: 1.7492\n",
            "Epoch 18/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 1.4853 - val_loss: 1.7401\n",
            "Epoch 19/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.4601 - val_loss: 1.7376\n",
            "Epoch 20/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.4355 - val_loss: 1.7289\n",
            "Epoch 21/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.4108 - val_loss: 1.7290\n",
            "Epoch 22/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.3870 - val_loss: 1.7253\n",
            "Epoch 23/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.3627 - val_loss: 1.7221\n",
            "Epoch 24/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.3389 - val_loss: 1.7193\n",
            "Epoch 25/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.3154 - val_loss: 1.7134\n",
            "Epoch 26/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.2915 - val_loss: 1.7141\n",
            "Epoch 27/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 1.2678 - val_loss: 1.7116\n",
            "Epoch 28/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.2444 - val_loss: 1.7109\n",
            "Epoch 29/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 1.2208 - val_loss: 1.7098\n",
            "Epoch 30/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 1.1983 - val_loss: 1.7059\n",
            "Epoch 31/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.1755 - val_loss: 1.7030\n",
            "Epoch 32/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.1530 - val_loss: 1.7069\n",
            "Epoch 33/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.1299 - val_loss: 1.7021\n",
            "Epoch 34/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.1077 - val_loss: 1.7009\n",
            "Epoch 35/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.0850 - val_loss: 1.7038\n",
            "Epoch 36/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.0626 - val_loss: 1.6990\n",
            "Epoch 37/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.0407 - val_loss: 1.6995\n",
            "Epoch 38/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.0195 - val_loss: 1.7027\n",
            "Epoch 39/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.9977 - val_loss: 1.7059\n",
            "Epoch 40/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.9769 - val_loss: 1.7058\n",
            "Epoch 41/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.9562 - val_loss: 1.7011\n",
            "Epoch 42/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.9346 - val_loss: 1.7099\n",
            "Epoch 43/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.9142 - val_loss: 1.7068\n",
            "Epoch 44/100\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.8935 - val_loss: 1.7084\n",
            "Epoch 45/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.8746 - val_loss: 1.7077\n",
            "Epoch 46/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.8545 - val_loss: 1.7073\n",
            "Epoch 47/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.8353 - val_loss: 1.7073\n",
            "Epoch 48/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.8168 - val_loss: 1.7106\n",
            "Epoch 49/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.7982 - val_loss: 1.7111\n",
            "Epoch 50/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.7796 - val_loss: 1.7156\n",
            "Epoch 51/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.7616 - val_loss: 1.7125\n",
            "Epoch 52/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.7434 - val_loss: 1.7170\n",
            "Epoch 53/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.7262 - val_loss: 1.7278\n",
            "Epoch 54/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.7086 - val_loss: 1.7246\n",
            "Epoch 55/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.6923 - val_loss: 1.7266\n",
            "Epoch 56/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.6754 - val_loss: 1.7286\n",
            "Epoch 57/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.6588 - val_loss: 1.7349\n",
            "Epoch 58/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.6422 - val_loss: 1.7405\n",
            "Epoch 59/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.6273 - val_loss: 1.7372\n",
            "Epoch 60/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.6116 - val_loss: 1.7454\n",
            "Epoch 61/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.5960 - val_loss: 1.7467\n",
            "Epoch 62/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.5815 - val_loss: 1.7450\n",
            "Epoch 63/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.5672 - val_loss: 1.7532\n",
            "Epoch 64/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.5523 - val_loss: 1.7537\n",
            "Epoch 65/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.5385 - val_loss: 1.7587\n",
            "Epoch 66/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.5242 - val_loss: 1.7599\n",
            "Epoch 67/100\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.5107 - val_loss: 1.7746\n",
            "Epoch 68/100\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.4979 - val_loss: 1.7739\n",
            "Epoch 69/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4845 - val_loss: 1.7667\n",
            "Epoch 70/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.4721 - val_loss: 1.7674\n",
            "Epoch 71/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.4605 - val_loss: 1.7831\n",
            "Epoch 72/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4471 - val_loss: 1.7883\n",
            "Epoch 73/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4353 - val_loss: 1.7893\n",
            "Epoch 74/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4242 - val_loss: 1.7928\n",
            "Epoch 75/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4125 - val_loss: 1.7988\n",
            "Epoch 76/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.4010 - val_loss: 1.8039\n",
            "Epoch 77/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.3908 - val_loss: 1.7972\n",
            "Epoch 78/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.3796 - val_loss: 1.8090\n",
            "Epoch 79/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.3690 - val_loss: 1.8104\n",
            "Epoch 80/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.3594 - val_loss: 1.8174\n",
            "Epoch 81/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.3491 - val_loss: 1.8144\n",
            "Epoch 82/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.3398 - val_loss: 1.8198\n",
            "Epoch 83/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.3299 - val_loss: 1.8246\n",
            "Epoch 84/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.3209 - val_loss: 1.8244\n",
            "Epoch 85/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.3121 - val_loss: 1.8301\n",
            "Epoch 86/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.3035 - val_loss: 1.8413\n",
            "Epoch 87/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.2940 - val_loss: 1.8520\n",
            "Epoch 88/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.2857 - val_loss: 1.8530\n",
            "Epoch 89/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2776 - val_loss: 1.8549\n",
            "Epoch 90/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2700 - val_loss: 1.8601\n",
            "Epoch 91/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.2613 - val_loss: 1.8595\n",
            "Epoch 92/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.2548 - val_loss: 1.8615\n",
            "Epoch 93/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2467 - val_loss: 1.8787\n",
            "Epoch 94/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2400 - val_loss: 1.8669\n",
            "Epoch 95/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2330 - val_loss: 1.8816\n",
            "Epoch 96/100\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.2254 - val_loss: 1.8852\n",
            "Epoch 97/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.2186 - val_loss: 1.8890\n",
            "Epoch 98/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2130 - val_loss: 1.8935\n",
            "Epoch 99/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.2062 - val_loss: 1.8996\n",
            "Epoch 100/100\n",
            "44/44 [==============================] - 4s 99ms/step - loss: 0.2001 - val_loss: 1.9008\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d83e9b750>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally save model weights to file:\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "JtrTUcqY1ziq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Validation"
      ],
      "metadata": {
        "id": "Wvnjj_9x5nc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally load model weights from file if already trained:\n",
        "# WARNING: Any notebook parameters and the learned vocabulary are not \n",
        "# saved/loaded - i.e. this only makes sense when all other cells of the notebook\n",
        "# are run except for the model.fit\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "hrvP6ZvN1XuJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "#\n",
        "\n",
        "# Build encoder model for inferencing\n",
        "enc_model = Model(inputs=enc_inputs, outputs=enc_states, name='Inference_Encoder')\n",
        "enc_model.summary()\n",
        "\n",
        "# Build decoder model for inferencing\n",
        "dec_state_input_h = Input(shape=(lstm_units,))\n",
        "dec_state_input_c = Input(shape=(lstm_units,))\n",
        "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n",
        "dec_states = [state_h, state_c]\n",
        "dec_outputs = dec_dense(dec_outputs)\n",
        "dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states, name='Inference_Decoder')\n",
        "dec_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "5043292a-a14f-4540-f26d-58b3bda60a2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Inference_Encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Encoder_Input (InputLayer)  [(None, None)]            0         \n",
            "                                                                 \n",
            " Encoder_Embedding (Embeddin  (None, None, 200)        1200200   \n",
            " g)                                                              \n",
            "                                                                 \n",
            " Encoder_LSTM (LSTM)         [(None, 150),             210600    \n",
            "                              (None, 150),                       \n",
            "                              (None, 150)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,410,800\n",
            "Trainable params: 1,410,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"Inference_Decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 150),  210600      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 150),                     'input_1[0][0]',                \n",
            "                                 (None, 150)]                     'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   906151      ['Decoder_LSTM[1][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,316,951\n",
            "Trainable params: 2,316,951\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare question for inferencing\n",
        "\n",
        "def tokenize(question):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", question).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')"
      ],
      "metadata": {
        "id": "EWZxAUpDhNDm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict answer and compare to ground truth options\n",
        "\n",
        " def predict_answer(question, qa_df=None):\n",
        "    states_values = enc_model.predict(tokenize(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_answer = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_answer += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' or len(decoded_answer.split()) > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    # Skip START token\n",
        "    decoded_answer = decoded_answer[1:]\n",
        "\n",
        "    print(f'Original question: {question}')\n",
        "    print(f'Predicated answer: {decoded_answer}\\n')\n",
        "\n",
        "    if qa_df is not None:\n",
        "        # The following should contain all acceptable answers\n",
        "        reference_answers = qa_df.loc[qa_df['Question']==question, 'norm_answer'].to_list()\n",
        "        reference_answers = [ re.sub(' +', ' ', answer[8:-7]) for answer in reference_answers]\n",
        "        print(f'{reference_answers}')\n",
        "\n",
        "        # Calculate BLEU score: Note that little differences may result from e.g.\n",
        "        # spaces that were added to norm_answer when replacing punctuation earlier\n",
        "        bleu_score = sentence_bleu(reference_answers, decoded_answer, smoothing_function=SmoothingFunction().method0)\n",
        "        \n",
        "        print(f'BLEU score: {bleu_score}\\n')\n",
        "\n",
        "    else:\n",
        "        bleu_score = None\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "hhbE1b9wswR7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts:\n",
        "# Get 20 random numbers to choose random sentences and calculate BLEU score\n",
        "# per predicted answer but also on average\n",
        "\n",
        "def validate_predictions(qa_df):\n",
        "    bleu_total = 0\n",
        "    number_of_samples = min(20, len(qa_df.index))\n",
        "\n",
        "    for sample_question in qa_df['Question'].sample(number_of_samples, random_state=42):\n",
        "        bleu_total += predict_answer(sample_question, qa_df)\n",
        "\n",
        "    print(f'BLEU average for answers (n={number_of_samples}) = {bleu_total/number_of_samples}')"
      ],
      "metadata": {
        "id": "QObKQwyVLNzY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from actually trained questions\n",
        "\n",
        "print('Validating model against sample set from training questions\\n')\n",
        "validate_predictions(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5arKNX6vHZ7",
        "outputId": "c48f8308-c9a0-4e66-a5af-e93db8c7d9f0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from training questions\n",
            "\n",
            "Original question: what happened to stevie ray vaughan\n",
            "Predicated answer: stevie ray vaughan is widely considered to be one of the greatest musicians to come from the state of texas\n",
            "\n",
            "['stephen stevie ray vaughan october 3 1954 \\x80\\x93 august 27 1990 was an american guitarist singersongwriter and record producer', 'often referred to by his initials srv vaughan is best known as a founding member and leader of double trouble', 'in june 1989 in step was released and earned them a grammy award for best contemporary blues performance', 'stevie ray vaughan is widely considered to be one of the greatest musicians to come from the state of texas']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: What did the Supreme Court determine in Dred Scott v. Sandford?\n",
            "Predicated answer: the song was used as part of the house of the end of world war ii the third and was the first president of the united states and president of the united states of the united states of the state of texas\n",
            "\n",
            "['dred scott v sandford also known as the dred scott decision was a landmark decision by the us supreme court', 'the second ruling was that the federal government had no power to regulate slavery in any territory acquired subsequent to the creation of the united states', 'the decision played an important role in the timing of state secession and the civil war although it is extreme to say the decision caused the war', 'to which the court noted']\n",
            "BLEU score: 0.4251159677351853\n",
            "\n",
            "Original question: what spanish speaking countries have the most world cup titles\n",
            "Predicated answer: the championship has been awarded every four years since the inaugural tournament in 1930 except in 1942 and 1946 when it was not held because of the second world war\n",
            "\n",
            "['the championship has been awarded every four years since the inaugural tournament in 1930 except in 1942 and 1946 when it was not held because of the second world war', 'the current champions are spain who won the 2010 tournament', 'brazil have won five times and they are the only team to have played in every tournament']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what other territories did the english sharing the island of st kitts\n",
            "Predicated answer: the british west indies were the islands in and around the caribbean sea to the pacific\n",
            "\n",
            "['the british west indies were the islands in and around the caribbean that were part of the british empire', 'the remainder are british overseas territories', 'the remaining british west indies']\n",
            "BLEU score: 0.7206715716768533\n",
            "\n",
            "Original question: How Lisa Loeb\n",
            "Predicated answer: she was the first artist to have a top artist in the us state of us state and a product artist of all time\n",
            "\n",
            "['lisa anne loeb born march 11 1968 is an american singersongwriter and actress', 'she was the first artist to have a number one single in the united states while not signed to a recording contract']\n",
            "BLEU score: 0.5335607905293301\n",
            "\n",
            "Original question: what branch of army was dwight d eisenhower in\n",
            "Predicated answer: he was the running mate of dwight d eisenhower the republican party has been the first female created in the seller in the united states by the spanish government including the first states by the country or the european union by the european union\n",
            "\n",
            "['in 1951 he became the first supreme commander of nato', 'he otherwise left most political activity to his vice president richard nixon', 'he was a moderate conservative who continued new deal agencies and expanded social security', 'he also signed civil rights legislation in 1957 and 1960 to protect the right to vote', 'eisenhower is often ranked highly among the us presidents']\n",
            "BLEU score: 0.28223364694315145\n",
            "\n",
            "Original question: how many high schools are in the us\n",
            "Predicated answer: the term is true of the range of social history\n",
            "\n",
            "['education in the united states is mainly provided by the public sector with control and funding coming from three levels local state and federal in that order', 'educational standards and standardized testing decisions are usually made by state governments', 'the exact age range of students in these grade levels varies slightly from area to area', 'the year of 1910 also saw the first true high schools', 'special education was made into federal law in 1975']\n",
            "BLEU score: 0.6242478814880676\n",
            "\n",
            "Original question: what is general average sacrifice\n",
            "Predicated answer: american companies accepted it in 1949\n",
            "\n",
            "['american companies accepted it in 1949']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: when does v start\n",
            "Predicated answer: v is an american science fiction television series that ran for two seasons on abc from november 3 2009 to march 15 2012\n",
            "\n",
            "['v is an american science fiction television series that ran for two seasons on abc from november 3 2009 to march 15 2011']\n",
            "BLEU score: 0.9915604269282711\n",
            "\n",
            "Original question: what are geologists currently researching\n",
            "Predicated answer: currently geologists are also engaged in the same of climate change as they study the history and evidence of this group a origins\n",
            "\n",
            "['currently geologists are also engaged in the discussion of climate change as they study the history and evidence for this earth process']\n",
            "BLEU score: 0.8132094674340906\n",
            "\n",
            "Original question: what is hosting a website\n",
            "Predicated answer: a web hosting service is a type of internet hosting service that allows individuals and organizations to make their website accessible via the world wide web\n",
            "\n",
            "['a web hosting service is a type of internet hosting service that allows individuals and organizations to make their website accessible via the world wide web', 'the scope of web hosting services varies greatly', 'individuals and organizations may also obtain web page hosting from alternative service providers', 'single page hosting is generally sufficient for personal web pages']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: How Do You Find the mean of the squares of the first 10 counting numbers\n",
            "Predicated answer: the name square number comes from the name of the shape see below\n",
            "\n",
            "['in mathematics a square number or perfect square is an integer that is the square of an integer in other words it is the product of some integer with itself', 'for example 9 is a square number since it can be written as', 'the name square number comes from the name of the shape see below', 'the concept of square can be extended to some other number systems', 'starting with 1 there are square numbers up to and including where the expression represents the floor of the number']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what color is burgundy\n",
            "Predicated answer: at a diagram of the electromagnetic spectrum showing the electromagnetic spectrum or the range of frequencies or wavelengths\n",
            "\n",
            "['burgundy is a dark red color associated with the burgundy wine of the same name which in turn is named after the burgundy region of france']\n",
            "BLEU score: 0.25659432025137124\n",
            "\n",
            "Original question: who was the first person to climb the mount of the holy cross\n",
            "Predicated answer: the monument was transferred to the national park service in 1933\n",
            "\n",
            "['the monument was transferred to the national park service in 1933', 'this image is a reversed view of the mountain compared to how it actually appears']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what culture is mariah carey\n",
            "Predicated answer: in 2012 carey was ranked second on vh1 's list of the 100 greatest women in music\n",
            "\n",
            "[\"following the film's poor reception she was bought out of her recording contract for $50 million which led to a decline in her career\", 'its second single we belong together became her most successful single of the 2000s and was later named song of the decade by billboard', 'in a career spanning over two decades carey has sold more than 200 million records worldwide making her one of the bestselling music artists of all time', \"in 1998 she was honored as the world's bestselling recording artist of the 1990s at the world music awards\", 'carey was also named the bestselling female artist of the millennium in 2000', \"in 2012 carey was ranked second on vh1 's list of the 100 greatest women in music\"]\n",
            "BLEU score: 1.0\n",
            "\n",
            "Warning: out-of-vocabulary token '$10'\n",
            "Original question: who is on the $10. bill\n",
            "Predicated answer: the other is benjamin franklin on the 100 bill\n",
            "\n",
            "['the other is benjamin franklin on the $100 bill', 'hamilton is one of only four people featured on us paper currency 1861 to the present who was not born in the continental united states as he was from the west indies', 'all $10 bills issued today are federal reserve notes', 'approximately 6% of all us banknotes printed in 2009 were $10 bills']\n",
            "BLEU score: 0.949851806476473\n",
            "\n",
            "Original question: how is selena\n",
            "Predicated answer: she has sold over 60 million albums worldwide making her one of the bestselling artists of all time\n",
            "\n",
            "['on april 12 1995 two weeks after her death george w bush governor of texas at the time declared her birthday selena day in texas', 'she has sold over 60 million albums worldwide making her one of the bestselling artists of all time', 'she is also the only female artist to have five albums in us billboard 200 at the same time']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what does life insurance cover?\n",
            "Predicated answer: a common form of this design is term insurance\n",
            "\n",
            "['depending on the contract other events such as terminal illness or critical illness may also trigger payment', 'a common form of this design is term insurance', 'common forms in the us are whole life universal life and variable life policies']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who are the girls from the bad girls club?\n",
            "Predicated answer: the championship has been awarded every four years since the inaugural tournament in 1930 except in 1942 and 1946 when it was not held because of the second world war\n",
            "\n",
            "['the format of the show has changed from early seasons', 'there have been nine complete seasons of the bad girls club with the tenth season currently airing', \"the fourth season had the highest ratings in the show's history becoming its breakthrough season\", 'one of them love games bad girls need love too where past cast members seek true love has gained high ratings', 'oxygen has also released a mobile game comic strip and merchandise to promote the show', 'and the subsequent seasons of love games bad girls need love too']\n",
            "BLEU score: 0.38325986002145557\n",
            "\n",
            "Original question: what happened on april 11 1861 at fort sumter\n",
            "Predicated answer: the first half of the end was changed the following year to the first level to then\n",
            "\n",
            "['anderson was short of men food and supplies', 'following the battle there was widespread support from both north and south for further military action', 'the civil war had begun']\n",
            "BLEU score: 0.30898796706983067\n",
            "\n",
            "BLEU average for answers (n=20) = 0.7644646853277041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from test questions (i.e. unseen)\n",
        "\n",
        "print('Validating model against sample set from test questions:')\n",
        "print('''\n",
        "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
        "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
        "  ! ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
        "  ''')\n",
        "validate_predictions(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWE7XvQ6x5pW",
        "outputId": "ab675443-5fcc-4984-b3c7-6cf7ee444910"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from test questions:\n",
            "\n",
            "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
            "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
            "  ! ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
            "  \n",
            "Original question: what year was President kennedy president?\n",
            "Predicated answer: a president is frequently described as the most powerful person in the world\n",
            "\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.46132432565301634\n",
            "\n",
            "Original question: what countries did immigrants come from during the immigration\n",
            "Predicated answer: the series was founded in the end of the electoral college shows that was the first union on august 2 2009\n",
            "\n",
            "['the history of immigration to the united states is a continuing story of peoples from more populated continents particularly europe and also africa and asia crossing oceans to the new land', 'later africans were brought as slaves']\n",
            "BLEU score: 0.31243470387632505\n",
            "\n",
            "Original question: who owns disney\n",
            "Predicated answer: the company was ranked the most commonly used to march 1 1991\n",
            "\n",
            "['the walt disney studios the headquarters of the walt disney company', 'taking on its current name in 1986 it expanded its existing operations and also started divisions focused upon theater radio music publishing and online media', 'the company is best known for the products of its film studio the walt disney studios and today one of the largest and bestknown studios in hollywood', 'it also has a successful music division']\n",
            "BLEU score: 0.4732878582068027\n",
            "\n",
            "Original question: who won the 1967 nba championship\n",
            "Predicated answer: the national basketball association nba is a major professional basketball league in north america\n",
            "\n",
            "['this was the first championship series in 11 years without the boston celtics who were defeated in the division finals by philadelphia']\n",
            "BLEU score: 0.13222715968704338\n",
            "\n",
            "Original question: Where does the word baptism come from\n",
            "Predicated answer: his biggest success in mainstream music was in 1991 when an place on august 2 as top of the us billboard hot 100 chart\n",
            "\n",
            "['the new testament reports that jesus was baptized']\n",
            "BLEU score: 0.1151676029990094\n",
            "\n",
            "Original question: what year was the eiffel tower made\n",
            "Predicated answer: the first basketball team have based in the war from which was adapted to a poor market crash of her work\n",
            "\n",
            "['the tower has three levels for visitors', 'the walk from ground level to the first level is over 300 steps as is the walk from the first to the second level', 'the first and second levels have restaurants', 'the tower has become the most prominent symbol of both paris and france often in the establishing shot of films set in the city']\n",
            "BLEU score: 0.41475713436938744\n",
            "\n",
            "Original question: what is name of national anthem song of switzerland\n",
            "Predicated answer: it was one of the most influential members of the european union before the first time before her second role in the united states\n",
            "\n",
            "['this was because the council wanted the people to express their say on what they wanted as a national anthem']\n",
            "BLEU score: 0.25737160480542504\n",
            "\n",
            "Original question: what was 1933 like in the usa\n",
            "Predicated answer: it was the first of the first major league franchise of the american league\n",
            "\n",
            "['top left the tennessee valley authority part of the new deal being signed into law in 1933', 'the new deal was a series of economic programs enacted in the united states between 1933 and 1936', 'they involved presidential executive orders or laws passed by congress during the first term of president franklin d roosevelt', 'the final major items of new deal legislation were the creation of the united states housing authority and farm security administration both in 1937 and the fair labor standards act of 1938 which set maximum hours and minimum wages for most categories of workers', 'conservative republicans and democrats in congress joined in the informal conservative coalition', 'roosevelt himself turned his attention to the war effort and won reelection in 1940 and 1944', 'the largest programs still in existence today are the social security system and the securities and exchange commission sec']\n",
            "BLEU score: 0.5487199901933965\n",
            "\n",
            "Original question: how far is a league\n",
            "Predicated answer: a significant amount of life expectancy in the range of social and wavelengths was more than more than 1 in general or risk of its primary or cultural mass\n",
            "\n",
            "['a league is a unit of length or rarely area', 'it was long common in europe and latin america but it is no longer an official unit in any nation', 'the league originally referred to the distance a person could walk in an hour', 'since the middle ages many values have been specified in several countries']\n",
            "BLEU score: 0.38616635826057505\n",
            "\n",
            "Original question: what age do vietnam fish start to fight\n",
            "Predicated answer: the number of factors explain why the election are so close\n",
            "\n",
            "['by 2000 it had established diplomatic relations with most nations']\n",
            "BLEU score: 0.09624970369329527\n",
            "\n",
            "Original question: how many universities are in the united states\n",
            "Predicated answer: the other business is used by the same of the human body\n",
            "\n",
            "['education in the united states is mainly provided by the public sector with control and funding coming from three levels local state and federal in that order', 'educational standards and standardized testing decisions are usually made by state governments', 'the exact age range of students in these grade levels varies slightly from area to area', 'the year of 1910 also saw the first true high schools', 'special education was made into federal law in 1975']\n",
            "BLEU score: 0.5015278732185712\n",
            "\n",
            "Original question: where is money made in the united states\n",
            "Predicated answer: it is the second most powerful person in the united states of the united states and is the largest largest and in the world of the european union after london and paris\n",
            "\n",
            "['it is divided into 100 smaller units called cents', 'several countries use it as their official currency and in many others it is the de facto currency']\n",
            "BLEU score: 0.23090374750426254\n",
            "\n",
            "Original question: where was martin luther king shot?\n",
            "Predicated answer: he was one of the world's most important as a department of his history of the first day\n",
            "\n",
            "[\"king was planning a national occupation of washington dc called the poor people's campaign\", 'king was assassinated on april 4 1968 in memphis tennessee', 'his death was followed by riots in many us cities', 'king was awarded the presidential medal of freedom and the congressional gold medal posthumously', 'martin luther king jr day was established as a us federal holiday in 1986', 'hundreds of streets in the us have been renamed in his honor', 'a memorial statue on the national mall was opened to the public in 2011']\n",
            "BLEU score: 0.41960708936634034\n",
            "\n",
            "Original question: what are the names of the mountains and lakes in michigan\n",
            "Predicated answer: this is a list of people living in western california\n",
            "\n",
            "['this is a list of lakes and other bodies of water in michigan', 'the american state of michigan has four of the great lakes partially within its borders', 'many lakes bear the same name']\n",
            "BLEU score: 0.43994507238424885\n",
            "\n",
            "Original question: how many players on a side for a football game\n",
            "Predicated answer: the term field refers to the same as number of the social level is called september\n",
            "\n",
            "['american football known in the united states as football is a team sport', 'the team in possession of the ball the offense attempts to advance down the field by running with or passing the ball', 'otherwise they lose control of the ball to the opposing team', 'the team that has scored the most points by the end of the game wins', 'american football evolved from early forms of rugby particularly rugby union and association football soccer', 'american football is the most popular sport in the united states today and the national football league nfl is its most popular league']\n",
            "BLEU score: 0.6059742482129491\n",
            "\n",
            "Original question: when did texas become a state\n",
            "Predicated answer: it was the first major league in the united states of the united states\n",
            "\n",
            "['texas is the second most populous and the secondlargest of the 50 states in the united states of america and the largest state in the 48 contiguous united states', 'the term six flags over texas came from the several nations that had ruled over the territory', 'spain was the first european country to claim the area of texas', 'france held a shortlived colony in texas', 'a slave state texas declared its secession from the united states in early 1861 joining the confederate states of america during the american civil war', 'due to its long history as a center of the industry texas is associated with the image of the cowboy']\n",
            "BLEU score: 0.8646775002385805\n",
            "\n",
            "Original question: what is the si unit of pressure\n",
            "Predicated answer: the other is benjamin franklin on the 100 bill\n",
            "\n",
            "['pressure the symbol p is the ratio of force to the area over which that force is distributed']\n",
            "BLEU score: 0.10075024929909508\n",
            "\n",
            "Original question: how is the house of representatives close to the people?\n",
            "Predicated answer: the following is a list of colleges and universities in the united states of america and the first states to a major states of america and to november 3 2009\n",
            "\n",
            "['it is frequently referred to as the house', 'the other house is the senate', 'the composition and powers of the house are established in article one of the united states constitution', 'each us state is represented in the house in proportion to its population but is entitled to at least one representative', 'the house meets in the south wing of the united states capitol']\n",
            "BLEU score: 0.4396089554163385\n",
            "\n",
            "Original question: what percentage of water in in the body\n",
            "Predicated answer: the president is indirectly elected by the largest and is considered a second language of the largest and in the world where italy and the total population of mexico\n",
            "\n",
            "['in physiology body water is the water content of the human body', 'a significant fraction of the human body is water']\n",
            "BLEU score: 0.18986805671319307\n",
            "\n",
            "Original question: how many shows are filmed in a season for jersey shore\n",
            "Predicated answer: it was first used in many modern album particularly its other copies in other countries\n",
            "\n",
            "['jersey shore is an american reality television series which ran on mtv from december 3 2009 to december 20 2012 in the united states', 'the fourth season filmed in italy premiered on august 4 2011', 'the fifth season finale aired on march 15 2012', 'on march 19 2012 mtv confirmed that the series would return for their sixth season', 'on august 30 2012 mtv announced that the series will end after the sixth season which premiered on october 4', 'the series finale aired on december 20 2012']\n",
            "BLEU score: 0.35628752950976283\n",
            "\n",
            "BLEU average for answers (n=20) = 0.3673428381803809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Targeted validation\n",
        "Performed with three types of questions:\n",
        "* Question from actual training set\n",
        "* Question from test set (i.e. unseen) -> only to verify if 'a' answer is provided\n",
        "* Reworded questions from actual training set: demonstrate robustness"
      ],
      "metadata": {
        "id": "yv1ADVF354Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST CASE 1: Accurate question from actual training dataset')\n",
        "predict_answer('When was Purple Haze by Jimi Hendrix made?')\n",
        "\n",
        "print('TEST CASE 2: Accurate question from actual training dataset')\n",
        "predict_answer('How are glacier caves formed?')\n",
        "\n",
        "print('TEST CASE 3: Varying the question from the actual training dataset')\n",
        "predict_answer('How are caves of glaciers formed?')\n",
        "\n",
        "print('TEST CASE 4: Accurate question from actual training dataset')\n",
        "predict_answer('When was Apple Computer founded?')\n",
        "\n",
        "print('TEST CASE 5: Varying the question from the actual training dataset')\n",
        "predict_answer('When was Apple Computer incorporated?')\n",
        "\n",
        "print('TEST CASE 6: Unseen question from test set')\n",
        "predict_answer('How many players on a side for a football game?')\n",
        "\n",
        "print('TEST CASE 7: Varying the unseen question from test set')\n",
        "predict_answer('How many players per team in a football game?')\n",
        "\n",
        "print('TEST CASE 8: What where the most important factors that led to the defeat of the democrates in 1968?')\n",
        "predict_answer('What group took home the award for best rock album at the Australian Recording Industry Association (ARIA) Music Awards?')\n",
        "\n",
        "print('TEST CASE 9: Varying the long question from the training dataset')\n",
        "predict_answer('What group got the best rock album award at the Australian Recording Industry Association Music Awards?')"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh",
        "outputId": "6d669aeb-86d6-4748-cce8-9acd142cb62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST CASE 1: Accurate question from actual training dataset\n",
            "Original question: When was Purple Haze by Jimi Hendrix made?\n",
            "Predicated answer: the president has been been in the united states including the president and has been been in the united states\n",
            "\n",
            "TEST CASE 2: Accurate question from actual training dataset\n",
            "Original question: How are glacier caves formed?\n",
            "Predicated answer: a glacier cave is a cave formed within the ice of a glacier\n",
            "\n",
            "TEST CASE 3: Varying the question from the actual training dataset\n",
            "Original question: How are caves of glaciers formed?\n",
            "Predicated answer: the word is often used to distinguish between the european and australian but means made to many foods such as world or real foods such as such as a savings and money\n",
            "\n",
            "TEST CASE 4: Accurate question from actual training dataset\n",
            "Original question: When was Apple Computer founded?\n",
            "Predicated answer: the following is a list of nba players from the most populous state of the united states\n",
            "\n",
            "TEST CASE 5: Varying the question from the actual training dataset\n",
            "Original question: When was Apple Computer incorporated?\n",
            "Predicated answer: the show was its senior national park and team into the world series during their east in japan\n",
            "\n",
            "TEST CASE 6: Unseen question from test set\n",
            "Original question: How many players on a side for a football game?\n",
            "Predicated answer: the term field refers to the same as number of the social level is called september\n",
            "\n",
            "TEST CASE 7: Varying the unseen question from test set\n",
            "Original question: How many players per team in a football game?\n",
            "Predicated answer: the film has been translated into the world in new york and series were won their home in 1995\n",
            "\n",
            "TEST CASE 8: What where the most important factors that led to the defeat of the democrates in 1968?\n",
            "Original question: What group took home the award for best rock album at the Australian Recording Industry Association (ARIA) Music Awards?\n",
            "Predicated answer: for 2010 aria introduced public voted awards for the first time\n",
            "\n",
            "TEST CASE 9: Varying the long question from the training dataset\n",
            "Original question: What group got the best rock album award at the Australian Recording Industry Association Music Awards?\n",
            "Predicated answer: for example the president is also elected to the time of canada for the western western world\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual validation"
      ],
      "metadata": {
        "id": "Fej_U-ZsHAjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    test_case = input('Enter test case description, or enter \\'end\\' to stop: ')\n",
        "    if test_case == 'end':\n",
        "        break\n",
        "    question = input('Ask me something: ')\n",
        "\n",
        "    predict_answer(question)\n",
        "    print()"
      ],
      "metadata": {
        "outputId": "6f8a804d-412a-4db7-f6e1-c3db3efa6bc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmxSB-wNG_Jw"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter test case description, or enter 'end' to stop: end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# END OF NOTEBOOK\n",
        "---"
      ],
      "metadata": {
        "id": "Yix-x4lfy4QZ"
      }
    }
  ]
}
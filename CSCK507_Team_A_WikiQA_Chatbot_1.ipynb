{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyNPrO8PVI4b0u0uJeXIvocr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_WikiQA_Chatbot_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 1: LSTM without Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "\n",
        "Important note: \n",
        "The dataset includes incorrect answers, labelled accordingly. Learning from these can be switches on/off (see below).\n",
        "\n",
        "In a real setting, it would be sensible to add a concept called \"answer triggering\" and exclude learning from incorrect answers. Answer triggering  first assesses a question to qualify if the model may deliver a sensible answer - otherwise let the person know that the bot does not know. Ref: https://ieeexplore.ieee.org/document/8079800\n",
        "\n",
        "In this notebook, the default is set to learn from invalid answers. This leads to more data for learning and thus a greater awareness of how sentences are constructed. And sometimes in funny conversations like with a poorly hearing dialogue partner, who provides 'perfectly valid answers - but to a different question'."
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For future study, i.e. to be mentioned in report\n",
        "* Dropout on input, embedding layers or other places\n",
        "* Check if lemmatizing on question side improves performance\n",
        "* Check if word embedding (e.g. using Word2Vec or GloVe) on question (i.e. input side) improves performance (beware of out-of-vocab)\n",
        "* Use of token dropout for data augmentation: https://aclanthology.org/2020.coling-main.379.pdf\n",
        "\n",
        "For review, and potential reference:\n",
        "* Khin, N.N., Soe, K.M., 2020. Question Answering based University Chatbot using Sequence to Sequence Model, doi:10.1109/o-cocosda50338.2020.9295021"
      ],
      "metadata": {
        "id": "AHuZjEDChQ_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Configuration and framework"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = None  # set this to None if no limit on question length\n",
        "max_answer_tokens = None    # set this to None if no limit on answer length\n",
        "\n",
        "# Model parameters\n",
        "lstm_units = 150\n",
        "embedding_units = 200\n",
        "encoder_lstm_dropout = 0.2\n",
        "encoder_lstm_recurrent_dropout = 0.2\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 50\n",
        "number_of_epochs = 100"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cfe019-ca74-4898-85c5-27895d86db46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 15 15:05:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data acquisition and loading"
      ],
      "metadata": {
        "id": "Zd9YuELT4861"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "179b2114-3789-442f-ea95-7b6dd1c9182f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7f409d008690>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Dataset preparation (pre-processing, transformation)\n",
        "Note that no cleansing as such is required, as prior analysis has shown."
      ],
      "metadata": {
        "id": "ijtnhP1p5EaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "6069bc7f-1245-4a32-fc10-b43ed0b8aa89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers and count number of tokens\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower().strip() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower().strip()+' _STOP_' for s in df['Sentence']]\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop sentences which are too long\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    if max_question_tokens is not None:\n",
        "        df.drop(df[df['question_tokens']>max_question_tokens].index, inplace=True)\n",
        "    if max_answer_tokens is not None:\n",
        "        df.drop(df[df['answer_tokens']>max_answer_tokens+2].index, inplace=True)    "
      ],
      "metadata": {
        "id": "kSV824B9dt6K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    # Tokenize\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "\n",
        "    # Optionally remove sentences with out-of-vocabulary tokens\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "8b6e77c3-ec43-4909-cb47-ee923b624551"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc18e91c-32b8-43c9-dd4e-7a44202e2ef5",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2181\n",
            "Net validation dataset size: 108\n",
            "Net test dataset size: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data for training and validation by aligning lengths (i.e. padding)\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {train_decoder_output_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9CFhVHscxG5",
        "outputId": "327ff786-e3c0-4de6-cca9-c229fd62cfec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2181, 21)\n",
            "Decoder input data shape: (2181, 52)\n",
            "Decoder output data shape: (2181, 52, 6001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Modelling and training"
      ],
      "metadata": {
        "id": "-rPOfWDC5ikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "# Input layer for encoder\n",
        "enc_inputs = Input(shape=(None,), name='Encoder_Input')\n",
        "\n",
        "# Embedding layer for encoder\n",
        "enc_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, \n",
        "                          name='Encoder_Embedding')(enc_inputs)\n",
        "\n",
        "# LSTM layer for encoder\n",
        "_, state_h, state_c = LSTM(lstm_units, return_state=True, \n",
        "                           dropout=encoder_lstm_dropout,\n",
        "                           recurrent_dropout=encoder_lstm_recurrent_dropout,\n",
        "                           name='Encoder_LSTM')(enc_embedding)\n",
        "\n",
        "# Combine states from encoder LSTM layer\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "# Input layer for decoder\n",
        "dec_inputs = Input(shape=(None,), name='Decoder_Input')\n",
        "\n",
        "# Embedding layer for decoder\n",
        "dec_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, name='Decoder_Embedding')(dec_inputs)\n",
        "\n",
        "# LSTM layer for decoder\n",
        "dec_lstm = LSTM(lstm_units, return_state=True, return_sequences=True, name='Decoder_LSTM')\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "\n",
        "# Dense layer for decoder\n",
        "dec_dense = Dense(vocab_size, activation=softmax, name='Decoder_Dense')\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "# Summarised printout\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "d8c00036-995e-4832-bbcc-3f6542d491c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer Encoder_LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Encoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Encoder_LSTM (LSTM)            [(None, 150),        210600      ['Encoder_Embedding[0][0]']      \n",
            "                                 (None, 150),                                                     \n",
            "                                 (None, 150)]                                                     \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 150),  210600      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 150),                     'Encoder_LSTM[0][1]',           \n",
            "                                 (None, 150)]                     'Encoder_LSTM[0][2]']           \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   906151      ['Decoder_LSTM[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,727,751\n",
            "Trainable params: 3,727,751\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=batch_size, epochs=number_of_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "1b0baafc-2bd8-4e07-e261-2182c8f6ada1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "44/44 [==============================] - 13s 137ms/step - loss: 2.2213 - val_loss: 2.0369\n",
            "Epoch 2/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 2.0131 - val_loss: 1.9797\n",
            "Epoch 3/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.9501 - val_loss: 1.9449\n",
            "Epoch 4/100\n",
            "44/44 [==============================] - 5s 107ms/step - loss: 1.9020 - val_loss: 1.9168\n",
            "Epoch 5/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.8591 - val_loss: 1.8900\n",
            "Epoch 6/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.8188 - val_loss: 1.8681\n",
            "Epoch 7/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.7830 - val_loss: 1.8451\n",
            "Epoch 8/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.7510 - val_loss: 1.8253\n",
            "Epoch 9/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 1.7217 - val_loss: 1.8122\n",
            "Epoch 10/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.6938 - val_loss: 1.7993\n",
            "Epoch 11/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.6660 - val_loss: 1.7917\n",
            "Epoch 12/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.6400 - val_loss: 1.7798\n",
            "Epoch 13/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.6133 - val_loss: 1.7744\n",
            "Epoch 14/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 1.5880 - val_loss: 1.7678\n",
            "Epoch 15/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.5627 - val_loss: 1.7593\n",
            "Epoch 16/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.5385 - val_loss: 1.7520\n",
            "Epoch 17/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.5144 - val_loss: 1.7480\n",
            "Epoch 18/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 1.4893 - val_loss: 1.7428\n",
            "Epoch 19/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.4660 - val_loss: 1.7395\n",
            "Epoch 20/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.4425 - val_loss: 1.7348\n",
            "Epoch 21/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.4187 - val_loss: 1.7342\n",
            "Epoch 22/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.3947 - val_loss: 1.7284\n",
            "Epoch 23/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.3713 - val_loss: 1.7213\n",
            "Epoch 24/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.3471 - val_loss: 1.7189\n",
            "Epoch 25/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.3237 - val_loss: 1.7232\n",
            "Epoch 26/100\n",
            "44/44 [==============================] - 5s 106ms/step - loss: 1.3006 - val_loss: 1.7170\n",
            "Epoch 27/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.2768 - val_loss: 1.7180\n",
            "Epoch 28/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 1.2536 - val_loss: 1.7126\n",
            "Epoch 29/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.2301 - val_loss: 1.7115\n",
            "Epoch 30/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 1.2076 - val_loss: 1.7133\n",
            "Epoch 31/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.1845 - val_loss: 1.7076\n",
            "Epoch 32/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 1.1621 - val_loss: 1.7082\n",
            "Epoch 33/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.1400 - val_loss: 1.7052\n",
            "Epoch 34/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 1.1183 - val_loss: 1.7050\n",
            "Epoch 35/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 1.0966 - val_loss: 1.7090\n",
            "Epoch 36/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.0749 - val_loss: 1.7059\n",
            "Epoch 37/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 1.0532 - val_loss: 1.7057\n",
            "Epoch 38/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.0326 - val_loss: 1.7144\n",
            "Epoch 39/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 1.0116 - val_loss: 1.7109\n",
            "Epoch 40/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.9910 - val_loss: 1.7127\n",
            "Epoch 41/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.9709 - val_loss: 1.7154\n",
            "Epoch 42/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.9500 - val_loss: 1.7149\n",
            "Epoch 43/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.9302 - val_loss: 1.7141\n",
            "Epoch 44/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.9109 - val_loss: 1.7157\n",
            "Epoch 45/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.8908 - val_loss: 1.7172\n",
            "Epoch 46/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.8725 - val_loss: 1.7199\n",
            "Epoch 47/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.8526 - val_loss: 1.7216\n",
            "Epoch 48/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.8343 - val_loss: 1.7228\n",
            "Epoch 49/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.8168 - val_loss: 1.7222\n",
            "Epoch 50/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.7975 - val_loss: 1.7236\n",
            "Epoch 51/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.7795 - val_loss: 1.7288\n",
            "Epoch 52/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.7624 - val_loss: 1.7430\n",
            "Epoch 53/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.7446 - val_loss: 1.7361\n",
            "Epoch 54/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.7280 - val_loss: 1.7474\n",
            "Epoch 55/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.7107 - val_loss: 1.7465\n",
            "Epoch 56/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.6944 - val_loss: 1.7462\n",
            "Epoch 57/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.6789 - val_loss: 1.7518\n",
            "Epoch 58/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.6616 - val_loss: 1.7526\n",
            "Epoch 59/100\n",
            "44/44 [==============================] - 5s 106ms/step - loss: 0.6463 - val_loss: 1.7668\n",
            "Epoch 60/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.6307 - val_loss: 1.7628\n",
            "Epoch 61/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.6155 - val_loss: 1.7584\n",
            "Epoch 62/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.6016 - val_loss: 1.7774\n",
            "Epoch 63/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.5854 - val_loss: 1.7680\n",
            "Epoch 64/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.5713 - val_loss: 1.7754\n",
            "Epoch 65/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.5571 - val_loss: 1.7743\n",
            "Epoch 66/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.5434 - val_loss: 1.7888\n",
            "Epoch 67/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.5292 - val_loss: 1.7912\n",
            "Epoch 68/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.5158 - val_loss: 1.7912\n",
            "Epoch 69/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.5027 - val_loss: 1.7996\n",
            "Epoch 70/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.4892 - val_loss: 1.7965\n",
            "Epoch 71/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.4768 - val_loss: 1.8130\n",
            "Epoch 72/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.4650 - val_loss: 1.8094\n",
            "Epoch 73/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.4521 - val_loss: 1.8041\n",
            "Epoch 74/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.4409 - val_loss: 1.8152\n",
            "Epoch 75/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.4292 - val_loss: 1.8225\n",
            "Epoch 76/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.4173 - val_loss: 1.8286\n",
            "Epoch 77/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.4059 - val_loss: 1.8369\n",
            "Epoch 78/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.3953 - val_loss: 1.8247\n",
            "Epoch 79/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.3840 - val_loss: 1.8321\n",
            "Epoch 80/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.3745 - val_loss: 1.8433\n",
            "Epoch 81/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.3633 - val_loss: 1.8472\n",
            "Epoch 82/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.3544 - val_loss: 1.8466\n",
            "Epoch 83/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.3439 - val_loss: 1.8570\n",
            "Epoch 84/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.3338 - val_loss: 1.8654\n",
            "Epoch 85/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.3255 - val_loss: 1.8571\n",
            "Epoch 86/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.3155 - val_loss: 1.8596\n",
            "Epoch 87/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.3073 - val_loss: 1.8758\n",
            "Epoch 88/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2987 - val_loss: 1.8722\n",
            "Epoch 89/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.2895 - val_loss: 1.8943\n",
            "Epoch 90/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.2813 - val_loss: 1.8731\n",
            "Epoch 91/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.2733 - val_loss: 1.8964\n",
            "Epoch 92/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2655 - val_loss: 1.9008\n",
            "Epoch 93/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2582 - val_loss: 1.8961\n",
            "Epoch 94/100\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.2509 - val_loss: 1.9108\n",
            "Epoch 95/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2432 - val_loss: 1.9040\n",
            "Epoch 96/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.2355 - val_loss: 1.9121\n",
            "Epoch 97/100\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.2294 - val_loss: 1.9137\n",
            "Epoch 98/100\n",
            "44/44 [==============================] - 4s 101ms/step - loss: 0.2224 - val_loss: 1.9231\n",
            "Epoch 99/100\n",
            "44/44 [==============================] - 4s 102ms/step - loss: 0.2161 - val_loss: 1.9248\n",
            "Epoch 100/100\n",
            "44/44 [==============================] - 4s 100ms/step - loss: 0.2096 - val_loss: 1.9403\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f40d3fff710>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally save model weights to file:\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "JtrTUcqY1ziq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Validation"
      ],
      "metadata": {
        "id": "Wvnjj_9x5nc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally load model weights from file if already trained:\n",
        "# WARNING: Any notebook parameters and the learned vocabulary are not \n",
        "# saved/loaded - i.e. this only makes sense when all other cells of the notebook\n",
        "# are run except for the model.fit\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "hrvP6ZvN1XuJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "#\n",
        "\n",
        "# Build encoder model for inferencing\n",
        "enc_model = Model(inputs=enc_inputs, outputs=enc_states, name='Inference_Encoder')\n",
        "enc_model.summary()\n",
        "\n",
        "# Build decoder model for inferencing\n",
        "dec_state_input_h = Input(shape=(lstm_units,))\n",
        "dec_state_input_c = Input(shape=(lstm_units,))\n",
        "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n",
        "dec_states = [state_h, state_c]\n",
        "dec_outputs = dec_dense(dec_outputs)\n",
        "dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states, name='Inference_Decoder')\n",
        "dec_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "dbda180a-1a55-4e66-a121-ec45f7656c38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Inference_Encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Encoder_Input (InputLayer)  [(None, None)]            0         \n",
            "                                                                 \n",
            " Encoder_Embedding (Embeddin  (None, None, 200)        1200200   \n",
            " g)                                                              \n",
            "                                                                 \n",
            " Encoder_LSTM (LSTM)         [(None, 150),             210600    \n",
            "                              (None, 150),                       \n",
            "                              (None, 150)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,410,800\n",
            "Trainable params: 1,410,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"Inference_Decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 150),  210600      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 150),                     'input_1[0][0]',                \n",
            "                                 (None, 150)]                     'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   906151      ['Decoder_LSTM[1][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,316,951\n",
            "Trainable params: 2,316,951\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare question for inferencing\n",
        "\n",
        "def tokenize(question):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", question).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')"
      ],
      "metadata": {
        "id": "EWZxAUpDhNDm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict answer and compare to ground truth options\n",
        "\n",
        " def predict_answer(question, qa_df=None):\n",
        "    states_values = enc_model.predict(tokenize(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_answer = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_answer += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' or len(decoded_answer.split()) > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    # Skip START token\n",
        "    decoded_answer = decoded_answer[1:]\n",
        "\n",
        "    print(f'Original question: {question}')\n",
        "    print(f'Predicated answer: {decoded_answer}\\n')\n",
        "\n",
        "    if qa_df is not None:\n",
        "        # The following should contain all acceptable answers\n",
        "        reference_answers = qa_df.loc[qa_df['Question']==question, 'norm_answer'].to_list()\n",
        "        reference_answers = [answer[8:-7] for answer in reference_answers]\n",
        "        print(f'{reference_answers}')\n",
        "\n",
        "        # Calculate BLEU score: Note that little differences may result from e.g.\n",
        "        # spaces that were added to norm_answer when replacing punctuation earlier\n",
        "        bleu_score = sentence_bleu(reference_answers, decoded_answer, smoothing_function=SmoothingFunction().method0)\n",
        "        \n",
        "        print(f'BLEU score: {bleu_score}\\n')\n",
        "\n",
        "    else:\n",
        "        bleu_score = None\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "hhbE1b9wswR7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts:\n",
        "# Get 20 random numbers to choose random sentences and calculate BLEU score\n",
        "# per predicted answer but also on average\n",
        "\n",
        "def validate_predictions(qa_df):\n",
        "    bleu_total = 0\n",
        "    number_of_samples = min(20, len(qa_df.index))\n",
        "\n",
        "    for sample_question in qa_df['Question'].sample(number_of_samples):\n",
        "        bleu_total += predict_answer(sample_question, qa_df)\n",
        "\n",
        "    print(f'BLEU average for answers (n={number_of_samples}) = {bleu_total/number_of_samples}')"
      ],
      "metadata": {
        "id": "QObKQwyVLNzY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from actually trained questions\n",
        "\n",
        "print('Validating model against sample set from training questions\\n')\n",
        "validate_predictions(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5arKNX6vHZ7",
        "outputId": "82c8f70a-346f-4c51-faff-a4e1051dd06d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from training questions\n",
            "\n",
            "Original question: what are two languages in Nigeria?\n",
            "Predicated answer: there are hundreds of languages spoken in nigeria\n",
            "\n",
            "['there are hundreds of languages spoken in nigeria']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: when do solar eclipses happen?\n",
            "Predicated answer: an eclipse is a natural phenomenon\n",
            "\n",
            "['photo of 1999 total eclipse', 'an eclipse is a natural phenomenon']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how big or small a visible light can get\n",
            "Predicated answer: the spectrum does not used to contain the right of all parts of the world war and is often used to staple states in representatives of its soundtrack companies with the central development of machine tools\n",
            "\n",
            "['electromagnetic radiation in this range of wavelengths is called visible light or simply light', 'the spectrum does not however contain all the colors that the human eyes and brain can distinguish', 'colors containing only one wavelength are also called pure colors', 'many species can see light with frequencies outside the visible spectrum which is defined in terms of human vision', 'birds however can see some red wavelengths but not as many as humans']\n",
            "BLEU score: 0.43601158297352954\n",
            "\n",
            "Original question: when did coca cola first come out\n",
            "Predicated answer: the cocacola company has on occasion introduced other cola drinks under the coke brand name\n",
            "\n",
            "['the las vegas strip world of cocacola museum in 2003', 'it is produced by the cocacola company of atlanta  georgia  and is often referred to simply as coke a registered trademark of the cocacola company in the united states since march 27 1944', 'the cocacola company has on occasion introduced other cola drinks under the coke brand name']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how long was angelina on the jersey shore?\n",
            "Predicated answer: the series finale aired on december 20 2012\n",
            "\n",
            "['jersey shore is an american reality television series which ran on mtv from december 3 2009 to december 20 2012 in the united states', 'the fourth season filmed in italy  premiered on august 4 2011', 'the fifth season finale aired on march 15 2012', 'on march 19 2012 mtv confirmed that the series would return for their sixth season', 'on august 30 2012 mtv announced that the series will end after the sixth season which premiered on october 4', 'the series finale aired on december 20 2012']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who do I talk to about federal grants\n",
            "Predicated answer: however smaller grants may be provided by a single name eg municipal government\n",
            "\n",
            "['in order to receive a grant some form of grant writing often referred to as either a proposal or an application is usually required', 'other grants can be given to individuals such as victims of natural disasters or individuals who seek to open a small business', 'however smaller grants may be provided by a government agency eg municipal government']\n",
            "BLEU score: 0.849221965610832\n",
            "\n",
            "Original question: what is wheat grain found in?\n",
            "Predicated answer: this grain is grown on more than 40 million active solar depending on her fifth\n",
            "\n",
            "['this grain is grown on more land area than any other commercial food', 'world trade in wheat is greater than for all other crops combined']\n",
            "BLEU score: 0.5286958884440358\n",
            "\n",
            "Original question: what is the capital city of california.\n",
            "Predicated answer: in addition the university of california davis is located in nearby davis west of the capital\n",
            "\n",
            "['sacramento is the capital city of the us state of california and the seat of government of sacramento county', 'in addition the university of california davis is located in nearby davis  west of the capital']\n",
            "BLEU score: 0.9810485197284439\n",
            "\n",
            "Original question: when did boston braves move\n",
            "Predicated answer: the braves are the only franchise to the national football league league league\n",
            "\n",
            "['the atlanta braves are a professional baseball team based in atlanta', \"the braves are a member of the eastern division of major league baseball 's national league\", 'the braves have played in turner field since 1997', 'the braves advanced to the world series five times in the 1990s winning the title in 1995', 'the braves are the only mlb franchise to have won the world series in three different home cities', 'in 1953 the team moved to milwaukee wisconsin and became the milwaukee braves followed by the move to atlanta in 1966']\n",
            "BLEU score: 0.736946049956363\n",
            "\n",
            "Original question: what is a forward in soccer\n",
            "Predicated answer: this position requires intelligence speed and power both of execution and of thought to perform the role well\n",
            "\n",
            "['this position requires intelligence speed and power both of execution and of thought to perform the role well']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who is the girl in imagination movers\n",
            "Predicated answer: this page is about the imagination movers band\n",
            "\n",
            "['this page is about the imagination movers band', 'for information about the tv show starring the band see imagination movers tv series', 'imagination movers is a band formed in new orleans  louisiana  in 2003', 'the movers write and perform all the songs on their show', 'in october 2009 the imagination movers started the live from the idea warehouse concert tour 2009', 'several performance dates were moved to later in 2012 to benefit from the increased exposure of the imagination movers tv show now playing daily on disney junior']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how many people in world today\n",
            "Predicated answer: the highest rates of growth â global population increases above 18 per year â were seen briefly during the 1950s and for a longer period during the 1960s and 1970s\n",
            "\n",
            "['actual recorded population figures are in blue', 'the world population is the total number of living humans on earth', 'the highest rates of growth â\\x80\\x93 global population increases above 18% per year â\\x80\\x93 were seen briefly during the 1950s and for a longer period during the 1960s and 1970s', 'the growth rate peaked at 22% in 1963 and had declined to 11% by 2011']\n",
            "BLEU score: 0.9849514419386002\n",
            "\n",
            "Original question: when monopoly came out\n",
            "Predicated answer: by 1934 a board game had been created much like the version of monopoly sold by parker brothers and its related companies through the rest of the 20th century and into the 21st\n",
            "\n",
            "['by 1934 a board game had been created much like the version of monopoly sold by parker brothers and its related companies through the rest of the 20th century and into the 21st', \"the game's name remains a registered trademark of parker brothers as do its specific design elements\", 'starting in 1985 a new generation of spinoff board games and card games appeared on both sides of the atlantic ocean']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what is 9/11 bombings\n",
            "Predicated answer: the september 11 attacks also referred to as september 11 september 11th or 911 were a series of four coordinated terrorist attacks launched by the islamic terrorist group alqaeda upon the united states in new york city and the united states on the american civil war\n",
            "\n",
            "['the september 11 attacks also referred to as september 11 september 11th or 911 were a series of four coordinated terrorist attacks launched by the islamic terrorist group alqaeda upon the united states in new york city and the washington dc area on september 11 2001', 'the destruction of the twin towers and other properties caused serious damage to the economy of lower manhattan and had a significant effect on global markets']\n",
            "BLEU score: 0.9001301885315743\n",
            "\n",
            "Original question: what is the salary for a representative\n",
            "Predicated answer: the house meets in the south wing of the united states capitol\n",
            "\n",
            "['it is frequently referred to as the house', 'the other house is the senate', 'the composition and powers of the house are established in article one of the united states constitution', 'each us state is represented in the house in proportion to its population but is entitled to at least one representative', 'the house meets in the south wing of the united states capitol']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what do ancients myths explain\n",
            "Predicated answer: greek mythology has had an extensive influence on the culture arts and literature of western civilization and remains part of the world\n",
            "\n",
            "['greek mythology has had an extensive influence on the culture arts and literature of western civilization and remains part of western heritage and language']\n",
            "BLEU score: 0.8214940706556245\n",
            "\n",
            "Original question: how do africans view the slave trade\n",
            "Predicated answer: slave transport in africa\n",
            "\n",
            "['the main slave routes in medieval africa', 'slave transport in africa', 'europeans usually did not enter the interior regions', 'slavery in africa has existed throughout the continent for many centuries and continues in the current day']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: What was distinguished about the Stamp Act from previous taxes\n",
            "Predicated answer: the stamp act met great resistance in the colonies\n",
            "\n",
            "['the stamp act met great resistance in the colonies', 'opposition to the stamp act was not limited to the colonies']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how did the civil war end\n",
            "Predicated answer: the war had its origin in the issue of slavery especially the extension of slavery into the western territories\n",
            "\n",
            "['the war had its origin in the issue of slavery  especially the extension of slavery into the western territories', 'foreign powers did not intervene', \"in the 1860 presidential election  republicans led by abraham lincoln  opposed expanding slavery into united states' territories\", 'eight remaining slave states continued to reject calls for secession', 'a peace conference failed to find a compromise and both sides prepared for war', 'lincoln issued the emancipation proclamation  which made ending slavery a war goal', 'the american civil war was one of the earliest true industrial wars']\n",
            "BLEU score: 0.9841408951634946\n",
            "\n",
            "Original question: what is the largest major league sport in the world?\n",
            "Predicated answer: the nfl mlb nba and nhl are commonly referred to as the federal or other or the big overall\n",
            "\n",
            "['the major professional sports leagues or simply major leagues in the united states and canada are the highest professional competitions of team sports in the two countries', 'the term major league was first used in 1921 in reference to major league baseball mlb the top level of professional american baseball', 'today the major northern north america professional team sports leagues are major league baseball mlb the national basketball association nba the national football league nfl and the national hockey league nhl', 'the nfl mlb nba and nhl are commonly referred to as the big four', 'the nfl has 32 teams and the others have 30 each', 'the vast majority of major league teams are concentrated in the most populous metropolitan areas of the united states', 'baseball football hockey and soccer have had professional leagues for over 100 years early leagues such as the national association  ohio league  and national hockey association formed the basis of the modern mlb nfl and nhl respectively']\n",
            "BLEU score: 0.6687680913072563\n",
            "\n",
            "BLEU average for answers (n=20) = 0.8945704347154878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from test questions (i.e. unseen)\n",
        "\n",
        "print('Validating model against sample set from test questions:')\n",
        "print('''\n",
        "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
        "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
        "  ! ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
        "  ''')\n",
        "validate_predictions(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWE7XvQ6x5pW",
        "outputId": "b160bd1b-3d30-4d76-a9b5-a2c665c065a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from test questions:\n",
            "\n",
            "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
            "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
            "  ! ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
            "  \n",
            "Original question: how did John F. Kennedy die?\n",
            "Predicated answer: since his death in the presidency are also covers and has sold more than a single\n",
            "\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas  texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.5334710162094974\n",
            "\n",
            "Original question: when did texas become a state\n",
            "Predicated answer: it is bordered to the north american country in the united states\n",
            "\n",
            "['texas is the second most populous and the secondlargest of the 50 states in the united states of america  and the largest state in the 48 contiguous united states', 'the term  six flags over texas  came from the several nations that had ruled over the territory', 'spain was the first european country to claim the area of texas', 'france held a shortlived colony in texas', 'a slave state  texas declared its secession from the united states in early 1861 joining the confederate states of america during the american civil war', 'due to its long history as a center of the industry texas is associated with the image of the cowboy']\n",
            "BLEU score: 0.8229528074234791\n",
            "\n",
            "Original question: what years was the 18th century\n",
            "Predicated answer: the president is a member of the united states of the highest true national park broadcast in the american football league of internet from the super states in its charts in america\n",
            "\n",
            "['philosophy and science increased in prominence', 'western historians have occasionally defined the 18th century otherwise for the purposes of their work']\n",
            "BLEU score: 0.24264768862658115\n",
            "\n",
            "Original question: what religion is primary in africa?\n",
            "Predicated answer: the main differences are used in the same class\n",
            "\n",
            "['others practice traditional and folk religions', 'religious distribution in africa']\n",
            "BLEU score: 0.15015316911057822\n",
            "\n",
            "Original question: who owns disney\n",
            "Predicated answer: the company was ranked used in england cincinnati its eighth century most of dallas texas\n",
            "\n",
            "['the walt disney studios  the headquarters of the walt disney company', 'taking on its current name in 1986 it expanded its existing operations and also started divisions focused upon theater radio music publishing and online media', 'the company is best known for the products of its film studio the walt disney studios  and today one of the largest and bestknown studios in hollywood', 'it also has a successful music division']\n",
            "BLEU score: 0.49546154328479386\n",
            "\n",
            "Original question: what religion is primary in africa?\n",
            "Predicated answer: the main differences are used in the same class\n",
            "\n",
            "['others practice traditional and folk religions', 'religious distribution in africa']\n",
            "BLEU score: 0.15015316911057822\n",
            "\n",
            "Original question: what religion is church of christ\n",
            "Predicated answer: the main differences are referred to as a complex\n",
            "\n",
            "['the term church of christ may refer to', 'a body of christians who continue to use only the new testament as the source for christian doctrine and practice and who consider themselves to be part of the original church in contrast to orthodox christianity catholic christianity or protestant christianity', 'the eastern orthodox or roman catholic churches primarily used by members of these churches', 'churches of christ', 'christian churches and churches of christ', 'christian church disciples of christ', 'churches of christ in australia', 'other historically related groups', 'churches of christ in christian union', 'denominations with a shared heritage in the latter day saint movement  which include', 'and other denominations called the church of jesus christ']\n",
            "BLEU score: 0.5337714708543734\n",
            "\n",
            "Original question: where is kevin bacon from\n",
            "Predicated answer: v is a american science fiction\n",
            "\n",
            "['he currently stars on the fox television series the following', 'in 2003 bacon received a star on the hollywood walk of fame']\n",
            "BLEU score: 0.2311816347941857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original question: what is the main language in india\n",
            "Predicated answer: these rules are broadly characterized by their clients\n",
            "\n",
            "['however languages listed in the eighth schedule of the indian constitution are sometimes referred to without legal standing as the national languages of india', 'two contact languages have played an important role in the history of india  persian and english']\n",
            "BLEU score: 0.12890549886842087\n",
            "\n",
            "Original question: how many shows are filmed in a season for jersey shore\n",
            "Predicated answer: the following is a list of critics of the new deal\n",
            "\n",
            "['jersey shore is an american reality television series which ran on mtv from december 3 2009 to december 20 2012 in the united states', 'the fourth season filmed in italy  premiered on august 4 2011', 'the fifth season finale aired on march 15 2012', 'on march 19 2012 mtv confirmed that the series would return for their sixth season', 'on august 30 2012 mtv announced that the series will end after the sixth season which premiered on october 4', 'the series finale aired on december 20 2012']\n",
            "BLEU score: 0.39562596434581165\n",
            "\n",
            "Original question: what year was President kennedy president?\n",
            "Predicated answer: the term was founded in the united states and canada at the first season as the president of the united states and canada during the year after the european union to the pacific 12 was formed to 11 from the european union to the european union with the european union with the highest\n",
            "\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas  texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.26194766124813856\n",
            "\n",
            "Original question: what are the imperial and metric systems of measurements\n",
            "Predicated answer: the university of alabama at birmingham uab is the most populous of the united states\n",
            "\n",
            "['the system came into official use across the british empire', 'by the late 20th century most nations of the former empire had officially adopted the metric system as their main system of measurement although as of 2013 the united kingdom had only partially adopted it']\n",
            "BLEU score: 0.5068710790798477\n",
            "\n",
            "Original question: how close or far do you want to be to the standard deviation\n",
            "Predicated answer: the number of medical life or the cycle also significantly directly\n",
            "\n",
            "['a low standard deviation indicates that the data points tend to be very close to the mean  high standard deviation indicates that the data points are spread out over a large range of values', 'note however that for measurements with percentage as unit the standard deviation will have percentage points as unit', 'when only a sample of data from a population is available the standard deviation of the population can be estimated by a modified quantity called the sample standard deviation']\n",
            "BLEU score: 0.18146083419147238\n",
            "\n",
            "Original question: What killed Frank Sinatra?\n",
            "Predicated answer: club levels allow fans multiple levels of entertainment\n",
            "\n",
            "['francis albert frank sinatra  december 12 1915 â\\x80\\x93 may 14 1998 was an american singer and film actor', 'two years later however he came out of retirement and in 1973 recorded several albums scoring a top 40 hit with  theme from new york new york  in 1980']\n",
            "BLEU score: 0.1153405203090306\n",
            "\n",
            "Original question: what religion is church of christ\n",
            "Predicated answer: the main differences are referred to as a complex\n",
            "\n",
            "['the term church of christ may refer to', 'a body of christians who continue to use only the new testament as the source for christian doctrine and practice and who consider themselves to be part of the original church in contrast to orthodox christianity catholic christianity or protestant christianity', 'the eastern orthodox or roman catholic churches primarily used by members of these churches', 'churches of christ', 'christian churches and churches of christ', 'christian church disciples of christ', 'churches of christ in australia', 'other historically related groups', 'churches of christ in christian union', 'denominations with a shared heritage in the latter day saint movement  which include', 'and other denominations called the church of jesus christ']\n",
            "BLEU score: 0.5337714708543734\n",
            "\n",
            "Original question: where was john f kennedy born\n",
            "Predicated answer: since his death in talk the presidency are also called her sixth\n",
            "\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas  texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.5237858632918296\n",
            "\n",
            "Original question: when did texas become a state\n",
            "Predicated answer: it is bordered to the north american country in the united states\n",
            "\n",
            "['texas is the second most populous and the secondlargest of the 50 states in the united states of america  and the largest state in the 48 contiguous united states', 'the term  six flags over texas  came from the several nations that had ruled over the territory', 'spain was the first european country to claim the area of texas', 'france held a shortlived colony in texas', 'a slave state  texas declared its secession from the united states in early 1861 joining the confederate states of america during the american civil war', 'due to its long history as a center of the industry texas is associated with the image of the cowboy']\n",
            "BLEU score: 0.8229528074234791\n",
            "\n",
            "Original question: how do i transfer my stock from transfer agent\n",
            "Predicated answer: a diagram of the electromagnetic spectrum showing various properties across the range of frequencies and wavelengths\n",
            "\n",
            "['stock transfer agent is the term used in the united states and canada', 'transfer secretary is used in south africa']\n",
            "BLEU score: 0.20268929682833958\n",
            "\n",
            "Original question: what religion is church of christ\n",
            "Predicated answer: the main differences are referred to as a complex\n",
            "\n",
            "['the term church of christ may refer to', 'a body of christians who continue to use only the new testament as the source for christian doctrine and practice and who consider themselves to be part of the original church in contrast to orthodox christianity catholic christianity or protestant christianity', 'the eastern orthodox or roman catholic churches primarily used by members of these churches', 'churches of christ', 'christian churches and churches of christ', 'christian church disciples of christ', 'churches of christ in australia', 'other historically related groups', 'churches of christ in christian union', 'denominations with a shared heritage in the latter day saint movement  which include', 'and other denominations called the church of jesus christ']\n",
            "BLEU score: 0.5337714708543734\n",
            "\n",
            "Original question: when did texas become a state\n",
            "Predicated answer: it is bordered to the north american country in the united states\n",
            "\n",
            "['texas is the second most populous and the secondlargest of the 50 states in the united states of america  and the largest state in the 48 contiguous united states', 'the term  six flags over texas  came from the several nations that had ruled over the territory', 'spain was the first european country to claim the area of texas', 'france held a shortlived colony in texas', 'a slave state  texas declared its secession from the united states in early 1861 joining the confederate states of america during the american civil war', 'due to its long history as a center of the industry texas is associated with the image of the cowboy']\n",
            "BLEU score: 0.8229528074234791\n",
            "\n",
            "BLEU average for answers (n=20) = 0.4094933887066331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Targeted validation\n",
        "Performed with three types of questions:\n",
        "* Question from actual training set\n",
        "* Question from test set (i.e. unseen) -> only to verify if 'a' answer is provided\n",
        "* Reworded questions from actual training set: demonstrate robustness"
      ],
      "metadata": {
        "id": "yv1ADVF354Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST CASE 1: Accurate question from actual training dataset')\n",
        "predict_answer('How much are the Harry Potter movies worth?')\n",
        "\n",
        "print('TEST CASE 2: Accurate question from actual training dataset')\n",
        "predict_answer('When was Apple Computer founded?')\n",
        "\n",
        "print('TEST CASE 3: Varying the question from the actual training dataset')\n",
        "predict_answer('When was Apple founded?')\n",
        "\n",
        "print('TEST CASE 4: Varying the question from the actual training dataset')\n",
        "predict_answer('When was Apple Computer incorporated?')\n",
        "\n",
        "print('TEST CASE 5: Varying the question from the actual training dataset')\n",
        "predict_answer('When was Apple incorporated?')\n",
        "\n",
        "print('TEST CASE 6: Unseen question from test set')\n",
        "predict_answer('What division is tsu football?')\n",
        "\n",
        "print('TEST CASE 7: Varying the unseen question from test set')\n",
        "predict_answer('Which division is tsu football?')\n",
        "\n",
        "print('TEST CASE 8: Varying the unseen question from test set')\n",
        "predict_answer('Which division is tsu football?')\n",
        "\n",
        "print('TEST CASE 9: Long question from actual training dataset')\n",
        "predict_answer('What is the name of the six-part comic book mini-series based on a character in Call of Duty: Modern Warfare 2?')\n",
        "\n",
        "print('TEST CASE 10: Varying the long question from the training dataset')\n",
        "predict_answer('What is the title of the comic book series with a protagonist from Modern Warfare Two?')"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh",
        "outputId": "d79dd6cc-d88b-49bb-e873-55e857670f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST CASE 1: Accurate question from actual training dataset\n",
            "Original question: How much are the Harry Potter movies worth?\n",
            "Predicated answer: harry potter is a list of mobile technology which has sold over 400 million copies\n",
            "\n",
            "TEST CASE 2: Accurate question from actual training dataset\n",
            "Original question: When was Apple Computer founded?\n",
            "Predicated answer: the company was founded on april 1 1976 and incorporated as apple computer inc on january 3 1977\n",
            "\n",
            "TEST CASE 3: Varying the question from the actual training dataset\n",
            "Original question: When was Apple founded?\n",
            "Predicated answer: the company was founded on april 1 1976 and incorporated as apple computer inc on january 3 1977\n",
            "\n",
            "TEST CASE 4: Varying the question from the actual training dataset\n",
            "Original question: When was Apple Computer incorporated?\n",
            "Predicated answer: the company was ranked entirely praised by critics and gilgamesh has sold in several years including the bestselling artists of all time\n",
            "\n",
            "TEST CASE 5: Varying the question from the actual training dataset\n",
            "Original question: When was Apple incorporated?\n",
            "Predicated answer: the day is one of the company of the decade at one of the decade series of the decade on 80\n",
            "\n",
            "TEST CASE 6: Unseen question from test set\n",
            "Warning: out-of-vocabulary token 'tsu'\n",
            "Original question: What division is tsu football?\n",
            "Predicated answer: in addition the university of money davis is now used but to be renewed for a reduced scale\n",
            "\n",
            "TEST CASE 7: Varying the unseen question from test set\n",
            "Warning: out-of-vocabulary token 'tsu'\n",
            "Original question: Which division is tsu football?\n",
            "Predicated answer: this is a member of the us basketball team that the national basketball association of the united states since its communist states in the same day\n",
            "\n",
            "TEST CASE 8: Varying the unseen question from test set\n",
            "Warning: out-of-vocabulary token 'tsu'\n",
            "Original question: Which division is tsu football?\n",
            "Predicated answer: this is a member of the us basketball team that the national basketball association of the united states since its communist states in the same day\n",
            "\n",
            "TEST CASE 9: Long question from actual training dataset\n",
            "Original question: What is the name of the six-part comic book mini-series based on a character in Call of Duty: Modern Warfare 2?\n",
            "Predicated answer: several spinoff games have also been released\n",
            "\n",
            "TEST CASE 10: Varying the long question from the training dataset\n",
            "Original question: What is the title of the comic book series with a protagonist from Modern Warfare Two?\n",
            "Predicated answer: the video was nominated for two awards at the national football team based on mtv new york city series\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual validation"
      ],
      "metadata": {
        "id": "Fej_U-ZsHAjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    test_case = input('Enter test case description, or enter \\'end\\' to stop: ')\n",
        "    if test_case == 'end':\n",
        "        break\n",
        "    question = input('Ask me something: ')\n",
        "\n",
        "    predict_answer(question)\n",
        "    print()"
      ],
      "metadata": {
        "outputId": "5202fbb0-92f1-42e6-d815-0ab27419bd13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmxSB-wNG_Jw"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter test case description, or enter 'end' to stop: end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# END OF NOTEBOOK\n",
        "---"
      ],
      "metadata": {
        "id": "Yix-x4lfy4QZ"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyPFPSuOVzjoV+P/A6Yf45By",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_WikiQA_Chatbot_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 2: LSTM with Bahdanau Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Non-attention solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "Bahdanau addition inspired by https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "Luong attention extension inspired by https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb\n",
        "\n",
        "Important note: \n",
        "The dataset includes incorrect answers, labelled accordingly. Learning from these can be switched on/off (see below).\n",
        "\n",
        "In a real setting, it would be sensible to add a concept called \"answer triggering\" and exclude learning from incorrect answers. Answer triggering  first assesses a question to qualify if the model may deliver a sensible answer - otherwise let the person know that the bot does not know. Ref: https://ieeexplore.ieee.org/document/8079800\n",
        "\n",
        "In this notebook, the default is set to learn from invalid answers. This leads to more data for learning and thus a greater awareness of how sentences are constructed. And sometimes in funny conversations like with a poorly hearing dialogue partner, who provides 'perfectly valid answers - but to a different question'."
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Configuration and framework"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = 20    # set this to None if no limit on question length\n",
        "max_answer_tokens = 50      # set this to None if no limit on answer length\n",
        "\n",
        "# Model parameters\n",
        "lstm_units = 200\n",
        "embedding_units = 200\n",
        "encoder_lstm_dropout = 0.2\n",
        "encoder_lstm_recurrent_dropout = 0.2\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 50\n",
        "number_of_epochs = 200"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Layer, Embedding, LSTM, Dense, RepeatVector\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57583132-35ce-4772-a753-be858fa93b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data acquisition and loading"
      ],
      "metadata": {
        "id": "Zd9YuELT4861"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "1b7e58ee-2e3e-47ae-decf-177bcff251ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7f1c59f32190>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Dataset preparation (pre-processing, transformation)\n",
        "Note that no cleansing as such is required, as prior analysis has shown."
      ],
      "metadata": {
        "id": "ijtnhP1p5EaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "94d7e604-708e-4e0a-948b-3595dcea4244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers and count number of tokens\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower().strip() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower().strip()+' _STOP_' for s in df['Sentence']]\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop sentences which are too long\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    if max_question_tokens is not None:\n",
        "        df.drop(df[df['question_tokens']>max_question_tokens].index, inplace=True)\n",
        "    if max_answer_tokens is not None:\n",
        "        df.drop(df[df['answer_tokens']>max_answer_tokens+2].index, inplace=True)    "
      ],
      "metadata": {
        "id": "kSV824B9dt6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    # Tokenize\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "\n",
        "    # Optionally remove sentences with out-of-vocabulary tokens\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "7d4335d2-5e30-46a4-9285-3e668efac80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27875a2-2459-4cf3-9c6f-6f6e5b0783bb",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2197\n",
            "Net validation dataset size: 109\n",
            "Net test dataset size: 245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data for training and validation by aligning lengths (i.e. padding)\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {train_decoder_output_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9CFhVHscxG5",
        "outputId": "6831cfe8-dde5-4ab6-d1b5-c6b66e76f805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2197, 19)\n",
            "Decoder input data shape: (2197, 52)\n",
            "Decoder output data shape: (2197, 52, 6001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Modelling and training"
      ],
      "metadata": {
        "id": "-rPOfWDC5ikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")\n",
        "        \n",
        "class BahdanauAttention(Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "oDaGS9lNoj2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "# Input layer for encoder\n",
        "enc_inputs = Input(shape=(None,), name='Encoder_Input')\n",
        "\n",
        "# Embedding layer for encoder\n",
        "enc_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, \n",
        "                          name='Encoder_Embedding')(enc_inputs)\n",
        "\n",
        "\n",
        "\n",
        "# LSTM layer for encoder\n",
        "stack_h, state_h, state_c = LSTM(lstm_units, return_state=True, \n",
        "                                 dropout=encoder_lstm_dropout,\n",
        "                                 recurrent_dropout=encoder_lstm_recurrent_dropout,\n",
        "                                 name='Encoder_LSTM')(enc_embedding)\n",
        "\n",
        "\n",
        "\n",
        "# Combine states from encoder LSTM layer\n",
        "enc_states = [stack_h, state_h, state_c]\n",
        "\n",
        "\n",
        "# -START ----------------------------------------------------\n",
        "#query_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n",
        "#    [query_seq_encoding, value_seq_encoding])\n",
        "# - END -----------------------------------------------------\n",
        "\n",
        "# Input layer for decoder\n",
        "dec_inputs = Input(shape=(None,), name='Decoder_Input')\n",
        "DECODER_input = RepeatVector(dec_inputs.shape[1])(state_h)\n",
        "print(DECODER_input)\n",
        "\n",
        "# Embedding layer for decoder\n",
        "dec_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, name='Decoder_Embedding')(dec_inputs)\n",
        "\n",
        "# LSTM layer for decoder\n",
        "dec_lstm = LSTM(lstm_units, return_state=True, return_sequences=True, name='Decoder_LSTM')\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "\n",
        "# Dense layer for decoder\n",
        "dec_dense = Dense(vocab_size, activation=softmax, name='Decoder_Dense')\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "# -START ----------------------------------------------------\n",
        "# Query embeddings of shape [batch_size, Tq, dimension].\n",
        "#query_embeddings = enc_embedding(enc_inputs)\n",
        "# Value embeddings of shape [batch_size, Tv, dimension].\n",
        "#value_embeddings = enc_embedding(dec_inputs)\n",
        "# - END -----------------------------------------------------\n",
        "\n",
        "# Compile the model\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "# Summarised printout\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "a43323b7-5f27-478f-e869-888e9b202458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f3bf771740b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Input layer for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdec_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Decoder_Input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mDECODER_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDECODER_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core/repeat_vector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Expected an integer value for `n`, got {type(n)}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected an integer value for `n`, got <class 'NoneType'>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stack_h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0QAMjJcx6mG",
        "outputId": "792c8663-1cbf-4e78-e039-2bd5d8737659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 200) dtype=float32 (created by layer 'Encoder_LSTM')>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=batch_size, epochs=number_of_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "38e78c8b-0db7-4a87-8d82-c5caebc9ddfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 12s 131ms/step - loss: 2.1968 - val_loss: 2.0253\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.9929 - val_loss: 1.9596\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.9257 - val_loss: 1.9220\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.8719 - val_loss: 1.8921\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.8253 - val_loss: 1.8625\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.7846 - val_loss: 1.8377\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.7485 - val_loss: 1.8177\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.7155 - val_loss: 1.8016\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.6840 - val_loss: 1.7896\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.6529 - val_loss: 1.7796\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.6210 - val_loss: 1.7668\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.5891 - val_loss: 1.7562\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 1.5575 - val_loss: 1.7427\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.5265 - val_loss: 1.7339\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.4955 - val_loss: 1.7272\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.4649 - val_loss: 1.7241\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 1.4341 - val_loss: 1.7116\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.4044 - val_loss: 1.7062\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.3742 - val_loss: 1.6989\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.3443 - val_loss: 1.6925\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.3145 - val_loss: 1.6888\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.2853 - val_loss: 1.6783\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.2561 - val_loss: 1.6748\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.2268 - val_loss: 1.6700\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.1985 - val_loss: 1.6671\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 1.1702 - val_loss: 1.6631\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.1426 - val_loss: 1.6594\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.1147 - val_loss: 1.6546\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.0879 - val_loss: 1.6552\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 1.0608 - val_loss: 1.6555\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 1.0343 - val_loss: 1.6522\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 1.0094 - val_loss: 1.6462\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.9833 - val_loss: 1.6436\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.9576 - val_loss: 1.6429\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.9333 - val_loss: 1.6482\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.9080 - val_loss: 1.6437\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.8843 - val_loss: 1.6431\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.8601 - val_loss: 1.6429\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.8369 - val_loss: 1.6471\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.8147 - val_loss: 1.6444\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.7919 - val_loss: 1.6458\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.7690 - val_loss: 1.6514\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.7476 - val_loss: 1.6498\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.7267 - val_loss: 1.6441\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.7052 - val_loss: 1.6509\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.6840 - val_loss: 1.6529\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.6644 - val_loss: 1.6548\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.6452 - val_loss: 1.6554\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.6251 - val_loss: 1.6606\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.6060 - val_loss: 1.6616\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.5876 - val_loss: 1.6647\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.5699 - val_loss: 1.6723\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.5528 - val_loss: 1.6776\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.5356 - val_loss: 1.6799\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.5176 - val_loss: 1.6779\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.5020 - val_loss: 1.6871\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.4853 - val_loss: 1.6814\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.4701 - val_loss: 1.6845\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.4536 - val_loss: 1.6916\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.4392 - val_loss: 1.6924\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.4244 - val_loss: 1.6935\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.4101 - val_loss: 1.6999\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.3968 - val_loss: 1.7084\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.3834 - val_loss: 1.7110\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.3695 - val_loss: 1.7217\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.3574 - val_loss: 1.7223\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.3443 - val_loss: 1.7305\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.3320 - val_loss: 1.7286\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.3200 - val_loss: 1.7349\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.3088 - val_loss: 1.7420\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.2982 - val_loss: 1.7433\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.2873 - val_loss: 1.7478\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.2759 - val_loss: 1.7594\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.2659 - val_loss: 1.7587\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.2564 - val_loss: 1.7676\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.2470 - val_loss: 1.7637\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.2380 - val_loss: 1.7794\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.2285 - val_loss: 1.7800\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.2202 - val_loss: 1.7832\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.2112 - val_loss: 1.7881\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.2037 - val_loss: 1.8015\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.1955 - val_loss: 1.8099\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1883 - val_loss: 1.8117\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1803 - val_loss: 1.8087\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.1734 - val_loss: 1.8224\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1664 - val_loss: 1.8263\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.1603 - val_loss: 1.8271\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.1540 - val_loss: 1.8453\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1473 - val_loss: 1.8392\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1417 - val_loss: 1.8503\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.1363 - val_loss: 1.8658\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.1306 - val_loss: 1.8647\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 4s 92ms/step - loss: 0.1256 - val_loss: 1.8731\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1210 - val_loss: 1.8763\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.1162 - val_loss: 1.8743\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1116 - val_loss: 1.8788\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1076 - val_loss: 1.8816\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.1028 - val_loss: 1.9085\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0987 - val_loss: 1.9150\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0957 - val_loss: 1.9166\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0920 - val_loss: 1.9234\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0881 - val_loss: 1.9292\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0853 - val_loss: 1.9367\n",
            "Epoch 104/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0823 - val_loss: 1.9306\n",
            "Epoch 105/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0791 - val_loss: 1.9389\n",
            "Epoch 106/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0762 - val_loss: 1.9538\n",
            "Epoch 107/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0738 - val_loss: 1.9610\n",
            "Epoch 108/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0717 - val_loss: 1.9535\n",
            "Epoch 109/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0690 - val_loss: 1.9713\n",
            "Epoch 110/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0669 - val_loss: 1.9594\n",
            "Epoch 111/200\n",
            "44/44 [==============================] - 4s 92ms/step - loss: 0.0650 - val_loss: 1.9623\n",
            "Epoch 112/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0627 - val_loss: 1.9878\n",
            "Epoch 113/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0607 - val_loss: 1.9859\n",
            "Epoch 114/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0589 - val_loss: 1.9943\n",
            "Epoch 115/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0578 - val_loss: 1.9999\n",
            "Epoch 116/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0558 - val_loss: 2.0039\n",
            "Epoch 117/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.0543 - val_loss: 2.0284\n",
            "Epoch 118/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0531 - val_loss: 2.0251\n",
            "Epoch 119/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0519 - val_loss: 2.0289\n",
            "Epoch 120/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0502 - val_loss: 2.0369\n",
            "Epoch 121/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0493 - val_loss: 2.0368\n",
            "Epoch 122/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0483 - val_loss: 2.0432\n",
            "Epoch 123/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0472 - val_loss: 2.0511\n",
            "Epoch 124/200\n",
            "44/44 [==============================] - 4s 92ms/step - loss: 0.0460 - val_loss: 2.0452\n",
            "Epoch 125/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0448 - val_loss: 2.0586\n",
            "Epoch 126/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0439 - val_loss: 2.0666\n",
            "Epoch 127/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0431 - val_loss: 2.0849\n",
            "Epoch 128/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0426 - val_loss: 2.0927\n",
            "Epoch 129/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0417 - val_loss: 2.0894\n",
            "Epoch 130/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0414 - val_loss: 2.0815\n",
            "Epoch 131/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0400 - val_loss: 2.0869\n",
            "Epoch 132/200\n",
            "44/44 [==============================] - 4s 91ms/step - loss: 0.0396 - val_loss: 2.0881\n",
            "Epoch 133/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0390 - val_loss: 2.0987\n",
            "Epoch 134/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0388 - val_loss: 2.1034\n",
            "Epoch 135/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0380 - val_loss: 2.1192\n",
            "Epoch 136/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0376 - val_loss: 2.1305\n",
            "Epoch 137/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0375 - val_loss: 2.1411\n",
            "Epoch 138/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0367 - val_loss: 2.1401\n",
            "Epoch 139/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0363 - val_loss: 2.1418\n",
            "Epoch 140/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0357 - val_loss: 2.1377\n",
            "Epoch 141/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0354 - val_loss: 2.1559\n",
            "Epoch 142/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0351 - val_loss: 2.1538\n",
            "Epoch 143/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0348 - val_loss: 2.1530\n",
            "Epoch 144/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0343 - val_loss: 2.1718\n",
            "Epoch 145/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0341 - val_loss: 2.1671\n",
            "Epoch 146/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0337 - val_loss: 2.1774\n",
            "Epoch 147/200\n",
            "44/44 [==============================] - 4s 97ms/step - loss: 0.0338 - val_loss: 2.1753\n",
            "Epoch 148/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0332 - val_loss: 2.1911\n",
            "Epoch 149/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0330 - val_loss: 2.1806\n",
            "Epoch 150/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0326 - val_loss: 2.1857\n",
            "Epoch 151/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0328 - val_loss: 2.1950\n",
            "Epoch 152/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0326 - val_loss: 2.1902\n",
            "Epoch 153/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0324 - val_loss: 2.1952\n",
            "Epoch 154/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0319 - val_loss: 2.1942\n",
            "Epoch 155/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0318 - val_loss: 2.1999\n",
            "Epoch 156/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0314 - val_loss: 2.2145\n",
            "Epoch 157/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0315 - val_loss: 2.2070\n",
            "Epoch 158/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0313 - val_loss: 2.2227\n",
            "Epoch 159/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0311 - val_loss: 2.2034\n",
            "Epoch 160/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0310 - val_loss: 2.2247\n",
            "Epoch 161/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0306 - val_loss: 2.2294\n",
            "Epoch 162/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0308 - val_loss: 2.2277\n",
            "Epoch 163/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0305 - val_loss: 2.2442\n",
            "Epoch 164/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0304 - val_loss: 2.2368\n",
            "Epoch 165/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0304 - val_loss: 2.2534\n",
            "Epoch 166/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0300 - val_loss: 2.2576\n",
            "Epoch 167/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0301 - val_loss: 2.2628\n",
            "Epoch 168/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0299 - val_loss: 2.2589\n",
            "Epoch 169/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0300 - val_loss: 2.2710\n",
            "Epoch 170/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0300 - val_loss: 2.2699\n",
            "Epoch 171/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0296 - val_loss: 2.2620\n",
            "Epoch 172/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0295 - val_loss: 2.2817\n",
            "Epoch 173/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0298 - val_loss: 2.2617\n",
            "Epoch 174/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0293 - val_loss: 2.2781\n",
            "Epoch 175/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0291 - val_loss: 2.2711\n",
            "Epoch 176/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0291 - val_loss: 2.3013\n",
            "Epoch 177/200\n",
            "44/44 [==============================] - 4s 98ms/step - loss: 0.0292 - val_loss: 2.2881\n",
            "Epoch 178/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0290 - val_loss: 2.3063\n",
            "Epoch 179/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0291 - val_loss: 2.2942\n",
            "Epoch 180/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0288 - val_loss: 2.3070\n",
            "Epoch 181/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0288 - val_loss: 2.3087\n",
            "Epoch 182/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0288 - val_loss: 2.3205\n",
            "Epoch 183/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0287 - val_loss: 2.3114\n",
            "Epoch 184/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0284 - val_loss: 2.3226\n",
            "Epoch 185/200\n",
            "44/44 [==============================] - 4s 92ms/step - loss: 0.0285 - val_loss: 2.3129\n",
            "Epoch 186/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0284 - val_loss: 2.3110\n",
            "Epoch 187/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0285 - val_loss: 2.3123\n",
            "Epoch 188/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0282 - val_loss: 2.3385\n",
            "Epoch 189/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0281 - val_loss: 2.3259\n",
            "Epoch 190/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0283 - val_loss: 2.3241\n",
            "Epoch 191/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0282 - val_loss: 2.3277\n",
            "Epoch 192/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0283 - val_loss: 2.3429\n",
            "Epoch 193/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0283 - val_loss: 2.3255\n",
            "Epoch 194/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0280 - val_loss: 2.3343\n",
            "Epoch 195/200\n",
            "44/44 [==============================] - 4s 96ms/step - loss: 0.0282 - val_loss: 2.3286\n",
            "Epoch 196/200\n",
            "44/44 [==============================] - 4s 94ms/step - loss: 0.0280 - val_loss: 2.3335\n",
            "Epoch 197/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0281 - val_loss: 2.3304\n",
            "Epoch 198/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0278 - val_loss: 2.3277\n",
            "Epoch 199/200\n",
            "44/44 [==============================] - 4s 95ms/step - loss: 0.0278 - val_loss: 2.3475\n",
            "Epoch 200/200\n",
            "44/44 [==============================] - 4s 93ms/step - loss: 0.0279 - val_loss: 2.3530\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc978a96710>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally save model weights to file:\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "JtrTUcqY1ziq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Validation"
      ],
      "metadata": {
        "id": "Wvnjj_9x5nc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally load model weights from file if already trained:\n",
        "# WARNING: Any notebook parameters and the learned vocabulary are not \n",
        "# saved/loaded - i.e. this only makes sense when all other cells of the notebook\n",
        "# are run except for the model.fit\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')"
      ],
      "metadata": {
        "id": "hrvP6ZvN1XuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "#\n",
        "\n",
        "# Build encoder model for inferencing\n",
        "enc_model = Model(inputs=enc_inputs, outputs=enc_states, name='Inference_Encoder')\n",
        "enc_model.summary()\n",
        "\n",
        "# Build decoder model for inferencing\n",
        "dec_state_input_h = Input(shape=(lstm_units,))\n",
        "dec_state_input_c = Input(shape=(lstm_units,))\n",
        "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n",
        "dec_states = [state_h, state_c]\n",
        "dec_outputs = dec_dense(dec_outputs)\n",
        "dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states, name='Inference_Decoder')\n",
        "dec_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "319871e4-0725-428e-85f0-73f190eff3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Inference_Encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Encoder_Input (InputLayer)  [(None, None)]            0         \n",
            "                                                                 \n",
            " Encoder_Embedding (Embeddin  (None, None, 200)        1200200   \n",
            " g)                                                              \n",
            "                                                                 \n",
            " Encoder_LSTM (LSTM)         [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,000\n",
            "Trainable params: 1,521,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"Inference_Decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 200),  320800      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 200),                     'input_1[0][0]',                \n",
            "                                 (None, 200)]                     'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   1206201     ['Decoder_LSTM[1][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,727,201\n",
            "Trainable params: 2,727,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare question for inferencing\n",
        "\n",
        "def tokenize(question):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", question).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')"
      ],
      "metadata": {
        "id": "EWZxAUpDhNDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict answer and compare to ground truth options\n",
        "\n",
        " def predict_answer(question, qa_df=None):\n",
        "    states_values = enc_model.predict(tokenize(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_answer = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_answer += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' or len(decoded_answer.split()) > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    # Skip START token\n",
        "    decoded_answer = decoded_answer[1:]\n",
        "\n",
        "    print(f'Original question: {question}')\n",
        "    print(f'Predicated answer: {decoded_answer}')\n",
        "\n",
        "    if qa_df is not None:\n",
        "        # The following should contain all acceptable answers\n",
        "        reference_answers = qa_df.loc[qa_df['Question']==question, 'norm_answer'].to_list()\n",
        "        reference_answers = [answer[8:-7] for answer in reference_answers]\n",
        "        print(f'{reference_answers}')\n",
        "\n",
        "        # Calculate BLEU score: Note that little differences may result from e.g.\n",
        "        # spaces that were added to norm_answer when replacing punctuation earlier\n",
        "        bleu_score = sentence_bleu(reference_answers, decoded_answer, smoothing_function=SmoothingFunction().method0)\n",
        "        \n",
        "        print(f'BLEU score: {bleu_score}\\n')\n",
        "\n",
        "    else:\n",
        "        bleu_score = None\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "hhbE1b9wswR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts:\n",
        "# Get 20 random numbers to choose random sentences and calculate BLEU score\n",
        "# per predicted answer but also on average\n",
        "\n",
        "def validate_predictions(qa_df):\n",
        "    bleu_total = 0\n",
        "    number_of_samples = min(20, len(qa_df.index))\n",
        "\n",
        "    for sample_question in qa_df['Question'].sample(number_of_samples):\n",
        "        bleu_total += predict_answer(sample_question, qa_df)\n",
        "\n",
        "    print(f'BLEU average for answers on trained questions (n={number_of_samples}) = {bleu_total/number_of_samples}')"
      ],
      "metadata": {
        "id": "QObKQwyVLNzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from actually trained questions\n",
        "\n",
        "print('Validating model against sample set from training questions\\n')\n",
        "validate_predictions(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5arKNX6vHZ7",
        "outputId": "d6bf44ec-213c-4402-b718-570d7ba7679c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from training questions\n",
            "\n",
            "Original question: where is diana prince from\n",
            "Predicated answer: diana prince is a fictional character appearing regularly in stories published by dc comics\n",
            "['diana prince is a fictional character appearing regularly in stories published by dc comics']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what is a league in the sea\n",
            "Predicated answer: the league originally referred to the distance a person could walk in an hour\n",
            "['a league is a unit of length or rarely area', 'it was long common in europe and latin america  but it is no longer an official unit in any nation', 'the league originally referred to the distance a person could walk in an hour', 'since the middle ages many values have been specified in several countries']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: when did the cold war start\n",
            "Predicated answer: the us and ussr became involved in political and military conflicts in the third world countries of latin america africa the middle east and southeast asia\n",
            "['it was an iconic symbol of the cold war and its fall in 1989 marked the end of the war', 'the us and ussr became involved in political and military conflicts in the third world countries of latin america  africa  the middle east  and southeast asia']\n",
            "BLEU score: 0.966210103165271\n",
            "\n",
            "Original question: what other territories did the english sharing the island of st kitts\n",
            "Predicated answer: the british west indies were the islands in and around the caribbean that were part of the british empire\n",
            "['the british west indies were the islands in and around the caribbean that were part of the british empire', 'the remainder are british overseas territories', 'the remaining british west indies']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what percentage of irish people get an education\n",
            "Predicated answer: the largest number of descendants of irish people live in the united states\n",
            "['there have been many notable irish people throughout history', 'there are descendants of irish people living in many western countries particularly in englishspeaking countries', 'the largest number of descendants of irish people live in the united states', 'the number of people living in australia who are of irish descent is higher as a percentage of total population than that of any other country', 'the very first inhabitants of iceland were irish']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what are square diamonds called?\n",
            "Predicated answer: a comparison\n",
            "['princess cut diamond set in a ring', 'the princess cut is a relatively new diamond cut having been created in the 1960s', 'a comparison']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how many schools are in the big ten\n",
            "Predicated answer: it is not to be confused with the big 12 conference which has only ten schools and represents a different region of the country\n",
            "['the big ten conference formerly western conference and big nine conference is the oldest division i college athletic conference in the united states', 'it is not to be confused with the big 12 conference  which has only ten schools and represents a different region of the country']\n",
            "BLEU score: 0.986149622543598\n",
            "\n",
            "Original question: what is the name of the founder of doha\n",
            "Predicated answer: located on the coast of the persian gulf\n",
            "['satellite view of doha', 'located on the coast of the persian gulf', 'the city of doha held the 2006 asian games  which was the largest asian games ever held']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what does 3g network mean\n",
            "Predicated answer: 3g short for third generation is the third generation of mobile telecommunications technology\n",
            "['3g short for third generation is the third generation of mobile telecommunications technology']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who put the bible in verses\n",
            "Predicated answer: the jewish divisions of the hebrew text differ at various points from those used by christians\n",
            "['the jewish divisions of the hebrew text differ at various points from those used by christians']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who was the first civilian to make a magnetic compass\n",
            "Predicated answer: a simple dry magnetic portable compass\n",
            "['a simple dry magnetic portable compass', 'a military compass that was used during world war i', 'the frame of reference defines the four cardinal directions or points \\x80\\x93 north  south  east  and west', 'usually a diagram called a compass rose  which shows the directions with their names usually abbreviated to initials is marked on the compass']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how did magic johnson get aids\n",
            "Predicated answer: after winning championships in high school and college johnson was selected first overall in the 1979 nba draft by the lakers\n",
            "['after winning championships in high school and college  johnson was selected first overall in the 1979 nba draft by the lakers', 'johnson was a member of the  dream team  the us basketball team that won the olympic gold medal in 1992', 'he was rated the greatest nba point guard of all time by espn in 2007']\n",
            "BLEU score: 0.9859268131509805\n",
            "\n",
            "Original question: who rules communism government\n",
            "Predicated answer: in the 21st century china and vietnam have allowed a mixed economy to develop\n",
            "['this period marked the greatest territorial extent of communist states', 'a communist state is a state where the means of production are collectively owned by the society', 'in practice communist states do not actually refer to themselves as such', 'communist states may have several legal political parties  but the communist party is usually granted a special or dominant role in government often by statute or under the constitution', 'in the 20th century most communist states adopted planned economies', 'in the 21st century china and vietnam have allowed a mixed economy to develop']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who is harrison ford married to\n",
            "Predicated answer: in 1997 ford was ranked no 1 in empire the top 100 movie stars of all time list\n",
            "['harrison ford born july 13 1942 is an american film actor and producer', 'he is famous for his performances as han solo in the original star wars trilogy and the title character of the indiana jones film series', 'in 1997 ford was ranked no 1 in empire the top 100 movie stars of all time list']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: who said \"A picture is worth a thousand words\"?\n",
            "Predicated answer: the expression use a picture\n",
            "['the expression use a picture', 'the december 8 1921 issue carries an ad entitled one look is worth a thousand words', 'a picture is worth a thousand words']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: when did wwi begin\n",
            "Predicated answer: the league of nations was formed in the hope of preventing another such conflict\n",
            "['ultimately more than 70 million military personnel including 60 million europeans were mobilised in one of the largest wars in history', 'within weeks the major powers were at war and via their colonies the conflict soon spread around the world', 'on 28 july the austrohungarians fired the first shots of the war as preparation for the invasion of serbia', 'italy and bulgaria went to war in 1915 and romania in 1916', 'the war ended in victory for the allies', 'the league of nations was formed in the hope of preventing another such conflict']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: what state was the civil war in\n",
            "Predicated answer: the war had its origin in the issue of slavery especially the extension of slavery into the western territories\n",
            "['the war had its origin in the issue of slavery  especially the extension of slavery into the western territories', 'foreign powers did not intervene', \"in the 1860 presidential election  republicans led by abraham lincoln  opposed expanding slavery into united states' territories\", 'eight remaining slave states continued to reject calls for secession', 'a peace conference failed to find a compromise and both sides prepared for war', 'the confederates assumed that european countries were so dependent on  king cotton  that they would intervene none did and none recognized the new confederate states of america', 'lincoln issued the emancipation proclamation  which made ending slavery a war goal', 'the american civil war was one of the earliest true industrial wars']\n",
            "BLEU score: 0.9841408951634946\n",
            "\n",
            "Original question: when does college football training camp start\n",
            "Predicated answer: training camp is divided into several different components\n",
            "[\"this is similar to baseball 's spring training\", 'training camp is used in several different ways', 'training camp is divided into several different components', 'the latter half of training camp leads directly into the exhibition season']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how many presidents of the us\n",
            "Predicated answer: the president must be at least 35 years of age and a natural born citizen of the united states\n",
            "[\"the white house  the president's official residence and center of the administration\", 'under the united states constitution  the president of the united states is the head of state and head of government of the united states', 'as chief of the executive branch and head of the federal government as a whole the presidency is the highest political office in the united states by influence and recognition', 'the president must be at least 35 years of age and a natural born citizen of the united states', 'for american leaders before this ratification see president of the continental congress', 'john f kennedy has been the only president of roman catholic faith and the current president barack obama  is the first president of african descent']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how many albums has eminem sold in his career\n",
            "Predicated answer: eminem is one of the bestselling artists in the world and is the bestselling artist of the 2000s\n",
            "['eminem is one of the bestselling artists in the world and is the bestselling artist of the 2000s', 'the same magazine declared him the king of hip hop', 'the album was a critical and commercial failure', 'in 2010 eminem released his seventh studio album recovery', 'recovery was an international success and was named the best selling album of 2010 worldwide joining the eminem show which was the best seller of 2002', 'eminem began an acting career in 2002 when he starred in the hip hop drama film 8 mile', 'he won the academy award for best original song  becoming the first rap artist ever to win the award']\n",
            "BLEU score: 1.0\n",
            "\n",
            "BLEU average for answers on trained questions (n=20) = 0.9961213717011672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from test questions (i.e. unseen)\n",
        "\n",
        "print('Validating model against sample set from test questions:')\n",
        "print('''\n",
        "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
        "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
        "  !ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
        "  ''')\n",
        "validate_predictions(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWE7XvQ6x5pW",
        "outputId": "374718b1-1d77-4e90-97d5-5733f0e6575f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating model against sample set from test questions:\n",
            "\n",
            "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
            "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
            "  !ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
            "  \n",
            "Original question: what was colonial government like\n",
            "Predicated answer: in the 20th century memorial day had a permanent amount for economic activities and the same earlier in 1933\n",
            "['by the time of the american revolution in 1775 most of these features applied to most of the colonies']\n",
            "BLEU score: 0.2444507336791804\n",
            "\n",
            "Original question: WHAT WAS THE WEATHER LIKE ON FEBRUARY 12, 1909\n",
            "Predicated answer: the us office of the video games in the united states air forces of following america two 4 and the 2008 greatest in the united states\n",
            "['january \\x80\\x93 february \\x80\\x93 march \\x80\\x93 april \\x80\\x93 may \\x80\\x93 june \\x80\\x93 july \\x80\\x93 august \\x80\\x93 september \\x80\\x93 october \\x80\\x93 november \\x80\\x93 december', 'the following events occurred in february 1909']\n",
            "BLEU score: 0.1948252875845229\n",
            "\n",
            "Original question: what religion is church of christ\n",
            "Predicated answer: the number of all time of english where the tower of the tower are more than 1 into a top types of values\n",
            "['the term church of christ may refer to', 'a body of christians who continue to use only the new testament as the source for christian doctrine and practice and who consider themselves to be part of the original church in contrast to orthodox christianity catholic christianity or protestant christianity', 'the eastern orthodox or roman catholic churches primarily used by members of these churches', 'churches of christ', 'christian churches and churches of christ', 'christian church disciples of christ', 'churches of christ in australia', 'other historically related groups', 'churches of christ in christian union', 'denominations with a shared heritage in the latter day saint movement  which include', 'and other denominations called the church of jesus christ']\n",
            "BLEU score: 0.4392794593039302\n",
            "\n",
            "Original question: what south american country is a colony\n",
            "Predicated answer: there are nearly people who watched the many language of american different and spanish\n",
            "['south america ranks fourth in area after asia  africa  and north america  and fifth in population after asia  africa  europe  and north america', 'most of the continent lies in the tropics']\n",
            "BLEU score: 0.3444952338996693\n",
            "\n",
            "Original question: what is the highest tax rate in the united states\n",
            "Predicated answer: from the time this is a list of all department of the united states government of government states that was the second president of that of mexico of the western population\n",
            "['an alternative tax applies at the federal and some state levels', 'due dates and other administrative procedures vary by jurisdiction']\n",
            "BLEU score: 0.18540426755478828\n",
            "\n",
            "Original question: how many amendments in us\n",
            "Predicated answer: the virginia and is the first natural used in the european system and the international economy at the planet often\n",
            "['the constitution of the united states is the supreme law of the united states of america', 'it went into effect on march 4 1789', 'the united states constitution can be changed through the amendment process']\n",
            "BLEU score: 0.3143384663989233\n",
            "\n",
            "Original question: what year was President kennedy president?\n",
            "Predicated answer: in 1968 he ran again for the presidency and was elected\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas  texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.6239518743862836\n",
            "\n",
            "Original question: where is kevin bacon from\n",
            "Predicated answer: heavy metal often referred to as metal is a genre of rock music that developed in the late 1960s and early 1970s largely in the united kingdom and in the united states\n",
            "['he currently stars on the fox television series the following', 'in 2003 bacon received a star on the hollywood walk of fame']\n",
            "BLEU score: 0.1661964117075223\n",
            "\n",
            "Original question: WHAT WAS THE WEATHER LIKE ON FEBRUARY 12, 1909\n",
            "Predicated answer: the us office of the video games in the united states air forces of following america two 4 and the 2008 greatest in the united states\n",
            "['january \\x80\\x93 february \\x80\\x93 march \\x80\\x93 april \\x80\\x93 may \\x80\\x93 june \\x80\\x93 july \\x80\\x93 august \\x80\\x93 september \\x80\\x93 october \\x80\\x93 november \\x80\\x93 december', 'the following events occurred in february 1909']\n",
            "BLEU score: 0.1948252875845229\n",
            "\n",
            "Original question: how many players on a side for a football game\n",
            "Predicated answer: the term mortality rate the number of deaths of children less than 5 years old per 1000 live births\n",
            "['american football known in the united states as football is a team sport', 'the team in possession of the ball the offense attempts to advance down the field by running with or passing the ball', 'otherwise they lose control of the ball to the opposing team', 'the team that has scored the most points by the end of the game wins', 'american football evolved from early forms of rugby  particularly rugby union  and association football soccer', 'american football is the most popular sport in the united states today and the national football league nfl is its most popular league']\n",
            "BLEU score: 0.33045511167757097\n",
            "\n",
            "Original question: when did texas become a state\n",
            "Predicated answer: the us was formed the new us of the third world colleges and is the first most populous metropolitan area in the united states\n",
            "['texas is the second most populous and the secondlargest of the 50 states in the united states of america  and the largest state in the 48 contiguous united states', 'the term  six flags over texas  came from the several nations that had ruled over the territory', 'spain was the first european country to claim the area of texas', 'france held a shortlived colony in texas', 'a slave state  texas declared its secession from the united states in early 1861 joining the confederate states of america during the american civil war', 'due to its long history as a center of the industry texas is associated with the image of the cowboy']\n",
            "BLEU score: 0.7264768324296398\n",
            "\n",
            "Original question: what is the acid in automobiles\n",
            "Predicated answer: the team that scores the most goals by the end of the match wins\n",
            "['with the plates restored to their original condition the process may now be repeated']\n",
            "BLEU score: 0.22026368394738868\n",
            "\n",
            "Original question: how many players on a side for a football game\n",
            "Predicated answer: the term mortality rate the number of deaths of children less than 5 years old per 1000 live births\n",
            "['american football known in the united states as football is a team sport', 'the team in possession of the ball the offense attempts to advance down the field by running with or passing the ball', 'otherwise they lose control of the ball to the opposing team', 'the team that has scored the most points by the end of the game wins', 'american football evolved from early forms of rugby  particularly rugby union  and association football soccer', 'american football is the most popular sport in the united states today and the national football league nfl is its most popular league']\n",
            "BLEU score: 0.33045511167757097\n",
            "\n",
            "Original question: what is thermal heat\n",
            "Predicated answer: the term estate in some the social be money of the 2000s and an official record of sedimentary card in the water\n",
            "['consequently heat is transported to earth as electromagnetic radiation', 'this is the main source of energy for life on earth']\n",
            "BLEU score: 0.181567748200244\n",
            "\n",
            "Original question: how are antibodies used in\n",
            "Predicated answer: it was developed by the united kingdom and canada in canada\n",
            "['though the general structure of all antibodies is very similar a small region at the tip of the protein is extremely variable allowing millions of antibodies with slightly different tip structures or antigen binding sites to exist', 'this allows a single antibody to be used by several different parts of the immune system']\n",
            "BLEU score: 0.20088206592796104\n",
            "\n",
            "Original question: who is criminal minds director\n",
            "Predicated answer: on march 14 2012 cbs renewed criminal minds for an eighth season which premiered on september 26 2012\n",
            "['on march 14 2012 cbs renewed criminal minds for an eighth season which premiered on september 26 2012']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: how did John F. Kennedy die?\n",
            "Predicated answer: kennedy was assassinated on november 22 1963 in dallas texas\n",
            "['thereafter he served in the us senate from 1953 until 1960', 'kennedy defeated vice president and republican candidate richard nixon in the 1960 us presidential election', 'kennedy was assassinated on november 22 1963 in dallas  texas', \"since the 1960s information concerning kennedy's private life has come to light\", 'kennedy ranks highly in public opinion ratings of us presidents']\n",
            "BLEU score: 0.9704997606532993\n",
            "\n",
            "Original question: how many shows are filmed in a season for jersey shore\n",
            "Predicated answer: the series opened in 2009 and is currently ongoing\n",
            "['jersey shore is an american reality television series which ran on mtv from december 3 2009 to december 20 2012 in the united states', 'the fourth season filmed in italy  premiered on august 4 2011', 'the fifth season finale aired on march 15 2012', 'on march 19 2012 mtv confirmed that the series would return for their sixth season', 'on august 30 2012 mtv announced that the series will end after the sixth season which premiered on october 4', 'the series finale aired on december 20 2012']\n",
            "BLEU score: 0.5760406397580399\n",
            "\n",
            "Original question: where was martin luther king shot?\n",
            "Predicated answer: king was assassinated on april 4 1968 in memphis tennessee\n",
            "[\"king was planning a national occupation of washington dc called the poor people's campaign\", 'king was assassinated on april 4 1968 in memphis tennessee', 'king was awarded the presidential medal of freedom and the congressional gold medal posthumously', 'martin luther king jr day was established as a us federal holiday in 1986', 'hundreds of streets in the us have been renamed in his honor', 'a memorial statue on the national mall was opened to the public in 2011']\n",
            "BLEU score: 1.0\n",
            "\n",
            "Original question: WHEN RUSSIA GET NUCLEAR BOMB\n",
            "Predicated answer: there is an ongoing debate about the use of nuclear energy and in america\n",
            "['russia is also party to the biological weapons convention and the chemical weapons convention']\n",
            "BLEU score: 0.1916375753473545\n",
            "\n",
            "BLEU average for answers on trained questions (n=20) = 0.4218022775859206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual validation\n",
        "Performed with three types of questions:\n",
        "* Question from actual training set\n",
        "* Question from test set (i.e. unseen) -> only to verify if 'a' answer is provided\n",
        "* Reworded questions from actual training set: demonstrate robustness"
      ],
      "metadata": {
        "id": "yv1ADVF354Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    test_case = input('Enter test case description, or enter \\'end\\' to stop: ')\n",
        "    if test_case == 'end':\n",
        "        break\n",
        "    question = input('Ask me something: ')\n",
        "\n",
        "    print(f'{predict_answer(question)}\\n')"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh",
        "outputId": "ef23cb30-101e-4e9e-8a1f-ea2fd3781209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter test case description, or enter 'end' to stop: test case 1: accurate question from actual training\n",
            "Ask me something: how much are the harry potter movies worth?\n",
            "Original question: how much are the harry potter movies worth?\n",
            "Predicated answer: harry potter is a series of seven fantasy novels written by the british author j k rowling\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 2: accurate question from actual training\n",
            "Ask me something: when was apple computer founded?\n",
            "Original question: when was apple computer founded?\n",
            "Predicated answer: the company was founded on april 1 1976 and incorporated as apple computer inc on january 3 1977\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 3: varying the question from training\n",
            "Ask me something: when was apple founded?\n",
            "Original question: when was apple founded?\n",
            "Predicated answer: the company was founded on april 1 1976 and incorporated as apple computer inc on january 3 1977\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 4: varying the question from training\n",
            "Ask me something: when was apple computer incorporated?\n",
            "Original question: when was apple computer incorporated?\n",
            "Predicated answer: the company was founded on april 1 1976 and incorporated as apple computer inc on january 3 1977\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 5: varying the question from training\n",
            "Ask me something: when was apple incorporated?\n",
            "Original question: when was apple incorporated?\n",
            "Predicated answer: the company was founded seven with the first sunday and is often referred to as simply as coke a registered trademark of the cocacola company\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 6: unseen question from test set\n",
            "Ask me something: what division is tsu football\n",
            "Warning: out-of-vocabulary token 'tsu'\n",
            "Original question: what division is tsu football\n",
            "Predicated answer: this is a complex of many 17 countries in new york city\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 7: varying the unseen question from the test set\n",
            "Ask me something: which division is tsu football?\n",
            "Warning: out-of-vocabulary token 'tsu'\n",
            "Original question: which division is tsu football?\n",
            "Predicated answer: the term has already been refer to its other role in volume\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 8: another unseen question from the test set\n",
            "Ask me something: how did crater lake get its color?\n",
            "Original question: how did crater lake get its color?\n",
            "Predicated answer: king of core public cities the majority of the president was not derived from the public and\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: test case 9: varying the unseen question from the test set\n",
            "Ask me something: how has crater lake gotten its color?\n",
            "Warning: out-of-vocabulary token 'gotten'\n",
            "Original question: how has crater lake gotten its color?\n",
            "Predicated answer: mexico officially the united mexican states is a us office of the national south and metropolitan area in the united states\n",
            "None\n",
            "\n",
            "Enter test case description, or enter 'end' to stop: end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# END OF NOTEBOOK\n",
        "---"
      ],
      "metadata": {
        "id": "Yix-x4lfy4QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2CkAOfd3y6ox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
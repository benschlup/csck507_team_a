{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyPVFo+aC6eiH9DkWfHzwghk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_WikiQA_Chatbot_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 2: LSTM with Bahdanau Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Non-attention solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "Bahdanau addition inspired by https://www.tensorflow.org/text/tutorials/nmt_with_attention  \n",
        "Luong attention extension inspired by https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb\n",
        "\n",
        "Important note: \n",
        "The dataset includes incorrect answers, labelled accordingly. Learning from these can be switched on/off (see below).\n",
        "\n",
        "In a real setting, it would be sensible to add a concept called \"answer triggering\" and exclude learning from incorrect answers. Answer triggering  first assesses a question to qualify if the model may deliver a sensible answer - otherwise let the person know that the bot does not know. Ref: https://ieeexplore.ieee.org/document/8079800\n",
        "\n",
        "In this notebook, the default is set to learn from invalid answers. This leads to more data for learning and thus a greater awareness of how sentences are constructed. And sometimes in funny conversations like with a poorly hearing dialogue partner, who provides 'perfectly valid answers - but to a different question'."
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Configuration and framework"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = None  # set this to None if no limit on question length\n",
        "max_answer_tokens = None    # set this to None if no limit on answer length\n",
        "\n",
        "# Model parameters\n",
        "lstm_units = 200\n",
        "embedding_units = 200\n",
        "encoder_lstm_dropout = 0.2\n",
        "encoder_lstm_recurrent_dropout = 0.2\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 50\n",
        "number_of_epochs = 200"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Layer, Embedding, LSTM, Dense, Concatenate\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "# For further use, if Luong should also be implemented:\n",
        "# requires a !pip install tensorflow_addons beforehand\n",
        "#from tensorflow_addons.seq2seq import LuongAttention, AttentionWrapper\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0509f177-6af7-4427-86e4-3b9efc54c004"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 15 13:46:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data acquisition and loading"
      ],
      "metadata": {
        "id": "Zd9YuELT4861"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "7cc5b803-85ca-4a31-f737-e3dbee1d9da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7f717a634210>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Dataset preparation (pre-processing, transformation)\n",
        "Note that no cleansing as such is required, as prior analysis has shown."
      ],
      "metadata": {
        "id": "ijtnhP1p5EaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "ba7d158a-9aab-4014-e5f4-836a82f28f43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers and count number of tokens\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower().strip() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower().strip()+' _STOP_' for s in df['Sentence']]\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop sentences which are too long\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    if max_question_tokens is not None:\n",
        "        df.drop(df[df['question_tokens']>max_question_tokens].index, inplace=True)\n",
        "    if max_answer_tokens is not None:\n",
        "        df.drop(df[df['answer_tokens']>max_answer_tokens+2].index, inplace=True)    "
      ],
      "metadata": {
        "id": "kSV824B9dt6K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    # Tokenize\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "\n",
        "    # Optionally remove sentences with out-of-vocabulary tokens\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "b7fa3791-bad8-430c-b604-3cf6c6738faf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fda4051-4c9d-4c49-ba66-24b0786da79f",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2181\n",
            "Net validation dataset size: 108\n",
            "Net test dataset size: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data for training and validation by aligning lengths (i.e. padding)\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {train_decoder_output_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9CFhVHscxG5",
        "outputId": "0d3168b4-de6a-4781-a9d1-40ffc7eccd8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2181, 21)\n",
            "Decoder input data shape: (2181, 52)\n",
            "Decoder output data shape: (2181, 52, 6001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Attention layer\n",
        "Bahdnau attention layer implemented on keras, full credit goes to  \n",
        "https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py"
      ],
      "metadata": {
        "id": "8Dlg1BMdrErY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "V-fOa2vKrE8Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Modelling and training"
      ],
      "metadata": {
        "id": "-rPOfWDC5ikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")\n",
        "        \n",
        "class BahdanauAttention(Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "oDaGS9lNoj2V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "# Input layer for encoder\n",
        "enc_inputs = Input(shape=(None,), name='Encoder_Input')\n",
        "\n",
        "# Embedding layer for encoder\n",
        "enc_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, \n",
        "                          name='Encoder_Embedding')(enc_inputs)\n",
        "\n",
        "# LSTM layer for encoder\n",
        "stack_h, state_h, state_c = LSTM(lstm_units, \n",
        "                                 return_state=True, \n",
        "                                 return_sequences=True,\n",
        "                                 dropout=encoder_lstm_dropout,\n",
        "                                 recurrent_dropout=encoder_lstm_recurrent_dropout,\n",
        "                                 name='Encoder_LSTM')(enc_embedding)\n",
        "\n",
        "\n",
        "\n",
        "# Combine states from encoder LSTM layer\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "# Input layer for decoder\n",
        "dec_inputs = Input(shape=(None,), name='Decoder_Input')\n",
        "\n",
        "# Embedding layer for decoder\n",
        "dec_embedding = Embedding(vocab_size, embedding_units, mask_zero=True, name='Decoder_Embedding')(dec_inputs)\n",
        "\n",
        "# LSTM layer for decoder\n",
        "dec_lstm = LSTM(lstm_units, \n",
        "                return_state=True, \n",
        "                return_sequences=True, \n",
        "                name='Decoder_LSTM')\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer()\n",
        "attn_op, attn_state = attn_layer([stack_h, dec_outputs])\n",
        "decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n",
        "\n",
        "# Dense layer for decoder\n",
        "dec_dense = Dense(vocab_size, activation=softmax, name='Decoder_Dense')\n",
        "output = dec_dense(decoder_concat_input)\n",
        "\n",
        "# Compile the model\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "# Summarised printout\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "177fe998-8ed3-4a15-b638-d641b39bbdce",
        "cellView": "code"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer Encoder_LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Encoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder_Embedding (Embedding)  (None, None, 200)    1200200     ['Decoder_Input[0][0]']          \n",
            "                                                                                                  \n",
            " Encoder_LSTM (LSTM)            [(None, None, 200),  320800      ['Encoder_Embedding[0][0]']      \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " Decoder_LSTM (LSTM)            [(None, None, 200),  320800      ['Decoder_Embedding[0][0]',      \n",
            "                                 (None, 200),                     'Encoder_LSTM[0][1]',           \n",
            "                                 (None, 200)]                     'Encoder_LSTM[0][2]']           \n",
            "                                                                                                  \n",
            " attention_layer (AttentionLaye  ((None, None, 200),  80200      ['Encoder_LSTM[0][0]',           \n",
            " r)                              (None, None, None)               'Decoder_LSTM[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, None, 400)    0           ['Decoder_LSTM[0][0]',           \n",
            "                                                                  'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " Decoder_Dense (Dense)          (None, None, 6001)   2406401     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,528,601\n",
            "Trainable params: 5,528,601\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=batch_size, epochs=number_of_epochs)"
      ],
      "metadata": {
        "id": "glC5E6w1M9mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d23c7c4-e315-4f1c-c506-4eba41f7605b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 29s 312ms/step - loss: 2.1958 - val_loss: 2.0157\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 1.9856 - val_loss: 1.9515\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 1.9143 - val_loss: 1.9112\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 1.8443 - val_loss: 1.8652\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 1.7869 - val_loss: 1.8344\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 1.7424 - val_loss: 1.8180\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.7023 - val_loss: 1.8043\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.6637 - val_loss: 1.7927\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.6244 - val_loss: 1.7847\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 1.5856 - val_loss: 1.7711\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 1.5463 - val_loss: 1.7621\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 1.5066 - val_loss: 1.7569\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 1.4674 - val_loss: 1.7475\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 1.4284 - val_loss: 1.7425\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.3918 - val_loss: 1.7423\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 1.3542 - val_loss: 1.7375\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 1.3174 - val_loss: 1.7305\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 1.2824 - val_loss: 1.7332\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.2467 - val_loss: 1.7216\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.2119 - val_loss: 1.7168\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 1.1781 - val_loss: 1.7169\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 1.1430 - val_loss: 1.7168\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 1.1091 - val_loss: 1.7091\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 1.0760 - val_loss: 1.7149\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 11s 252ms/step - loss: 1.0426 - val_loss: 1.7126\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 11s 252ms/step - loss: 1.0102 - val_loss: 1.7077\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.9775 - val_loss: 1.7075\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.9455 - val_loss: 1.7042\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.9144 - val_loss: 1.7022\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.8836 - val_loss: 1.7077\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.8526 - val_loss: 1.7052\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.8231 - val_loss: 1.7050\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.7945 - val_loss: 1.7026\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.7652 - val_loss: 1.7053\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.7393 - val_loss: 1.7109\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.7115 - val_loss: 1.7094\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.6857 - val_loss: 1.7089\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.6599 - val_loss: 1.7251\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.6357 - val_loss: 1.7224\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 0.6114 - val_loss: 1.7235\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.5895 - val_loss: 1.7281\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.5663 - val_loss: 1.7310\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 0.5437 - val_loss: 1.7348\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 0.5227 - val_loss: 1.7415\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 11s 261ms/step - loss: 0.5001 - val_loss: 1.7458\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.4818 - val_loss: 1.7437\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.4613 - val_loss: 1.7514\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.4433 - val_loss: 1.7626\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.4254 - val_loss: 1.7631\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.4074 - val_loss: 1.7569\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.3904 - val_loss: 1.7661\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.3741 - val_loss: 1.7779\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.3583 - val_loss: 1.7794\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 12s 280ms/step - loss: 0.3429 - val_loss: 1.7900\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 12s 276ms/step - loss: 0.3283 - val_loss: 1.7948\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.3137 - val_loss: 1.7992\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.3009 - val_loss: 1.8088\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.2873 - val_loss: 1.8108\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.2734 - val_loss: 1.8197\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.2620 - val_loss: 1.8189\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.2508 - val_loss: 1.8363\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.2385 - val_loss: 1.8404\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.2274 - val_loss: 1.8465\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.2165 - val_loss: 1.8496\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.2069 - val_loss: 1.8600\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 11s 260ms/step - loss: 0.1964 - val_loss: 1.8673\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.1887 - val_loss: 1.8762\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1786 - val_loss: 1.8894\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1709 - val_loss: 1.8921\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1633 - val_loss: 1.9059\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.1554 - val_loss: 1.9107\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.1478 - val_loss: 1.9126\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.1410 - val_loss: 1.9310\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.1340 - val_loss: 1.9338\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.1283 - val_loss: 1.9417\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 11s 258ms/step - loss: 0.1224 - val_loss: 1.9397\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1168 - val_loss: 1.9588\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1109 - val_loss: 1.9846\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.1064 - val_loss: 1.9732\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.1015 - val_loss: 1.9816\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.0971 - val_loss: 1.9818\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.0927 - val_loss: 2.0030\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.0886 - val_loss: 1.9971\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.0850 - val_loss: 2.0082\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.0816 - val_loss: 2.0264\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 11s 252ms/step - loss: 0.0786 - val_loss: 2.0118\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 11s 251ms/step - loss: 0.0755 - val_loss: 2.0480\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.0722 - val_loss: 2.0473\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.0696 - val_loss: 2.0526\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.0673 - val_loss: 2.0577\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.0650 - val_loss: 2.0739\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 11s 253ms/step - loss: 0.0624 - val_loss: 2.0889\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.0602 - val_loss: 2.0735\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 11s 255ms/step - loss: 0.0586 - val_loss: 2.0976\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 11s 251ms/step - loss: 0.0572 - val_loss: 2.1031\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.0552 - val_loss: 2.1018\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 11s 257ms/step - loss: 0.0535 - val_loss: 2.1155\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 11s 250ms/step - loss: 0.0520 - val_loss: 2.1133\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 11s 256ms/step - loss: 0.0509 - val_loss: 2.1245\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 11s 259ms/step - loss: 0.0495 - val_loss: 2.1192\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.0482 - val_loss: 2.1351\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 11s 254ms/step - loss: 0.0474 - val_loss: 2.1513\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 11s 251ms/step - loss: 0.0464 - val_loss: 2.1465\n",
            "Epoch 104/200\n",
            " 2/44 [>.............................] - ETA: 10s - loss: 0.0362"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally save model to file:\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model2.h5')\n",
        "#model.save_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model2_weights.h5')"
      ],
      "metadata": {
        "id": "JtrTUcqY1ziq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 7. Validation"
      ],
      "metadata": {
        "id": "Wvnjj_9x5nc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally load model weights from file if already trained:\n",
        "# WARNING: Any notebook parameters and the learned vocabulary are not \n",
        "# saved/loaded - i.e. this only makes sense when all other cells of the notebook\n",
        "# are run except for the model.fit\n",
        "#model.load('/content/drive/MyDrive/CSCK507_Team_A/qa_model2.h5')\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model2_weights.h5')"
      ],
      "metadata": {
        "id": "hrvP6ZvN1XuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "\n",
        "# Build encoder model for inferencing\n",
        "enc_model = Model(inputs=enc_inputs, outputs=[stack_h, enc_states], name='Inference_Encoder')\n",
        "enc_model.summary()\n",
        "\n",
        "# Build decoder model for inferencing\n",
        "dec_state_input_h = Input(shape=(lstm_units,))\n",
        "dec_state_input_c = Input(shape=(lstm_units,))\n",
        "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n",
        "\n",
        "dec_states = [state_h, state_c]\n",
        "\n",
        "## Komische Abweichung:\n",
        "dec_model = Model(inputs=[dec_inputs, dec_states_inputs], outputs=[dec_outputs] + dec_states, name='Inference_Decoder')\n",
        "\n",
        "dec_model.summary()"
      ],
      "metadata": {
        "id": "iHYIs3pL86Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare question for inferencing\n",
        "\n",
        "def tokenize(question):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", question).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')"
      ],
      "metadata": {
        "id": "EWZxAUpDhNDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict answer and compare to ground truth options\n",
        "\n",
        " def predict_answer(question, qa_df=None):\n",
        "    states_values = enc_model.predict(tokenize(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_answer = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_answer += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' or len(decoded_answer.split()) > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    # Skip START token\n",
        "    decoded_answer = decoded_answer[1:]\n",
        "\n",
        "    print(f'Original question: {question}')\n",
        "    print(f'Predicated answer: {decoded_answer}')\n",
        "\n",
        "    if qa_df is not None:\n",
        "        # The following should contain all acceptable answers\n",
        "        reference_answers = qa_df.loc[qa_df['Question']==question, 'norm_answer'].to_list()\n",
        "        reference_answers = [answer[8:-7] for answer in reference_answers]\n",
        "        print(f'{reference_answers}')\n",
        "\n",
        "        # Calculate BLEU score: Note that little differences may result from e.g.\n",
        "        # spaces that were added to norm_answer when replacing punctuation earlier\n",
        "        bleu_score = sentence_bleu(reference_answers, decoded_answer, smoothing_function=SmoothingFunction().method0)\n",
        "        \n",
        "        print(f'BLEU score: {bleu_score}\\n')\n",
        "\n",
        "    else:\n",
        "        bleu_score = None\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "hhbE1b9wswR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts:\n",
        "# Get 20 random numbers to choose random sentences and calculate BLEU score\n",
        "# per predicted answer but also on average\n",
        "\n",
        "def validate_predictions(qa_df):\n",
        "    bleu_total = 0\n",
        "    number_of_samples = min(20, len(qa_df.index))\n",
        "\n",
        "    for sample_question in qa_df['Question'].sample(number_of_samples):\n",
        "        bleu_total += predict_answer(sample_question, qa_df)\n",
        "\n",
        "    print(f'BLEU average for answers on trained questions (n={number_of_samples}) = {bleu_total/number_of_samples}')"
      ],
      "metadata": {
        "id": "QObKQwyVLNzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from actually trained questions\n",
        "\n",
        "print('Validating model against sample set from training questions\\n')\n",
        "validate_predictions(train_df)"
      ],
      "metadata": {
        "id": "Z5arKNX6vHZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate how the model predicts from test questions (i.e. unseen)\n",
        "\n",
        "print('Validating model against sample set from test questions:')\n",
        "print('''\n",
        "  ! NOTE THAT ASKING FOR ANSWERS ON UNSEEN QUESTIONS IS BARELY HELPFUL WITH\n",
        "  ! LITTLE DATASETS AND LITTLE VARIANCE ON BOTH Q/A SIDES:\n",
        "  !ADDING \"ANSWER TRIGGERING\" CONCEPT MAY BE PRUDENT\n",
        "  ''')\n",
        "validate_predictions(test_df)"
      ],
      "metadata": {
        "id": "hWE7XvQ6x5pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual validation\n",
        "Performed with three types of questions:\n",
        "* Question from actual training set\n",
        "* Question from test set (i.e. unseen) -> only to verify if 'a' answer is provided\n",
        "* Reworded questions from actual training set: demonstrate robustness"
      ],
      "metadata": {
        "id": "yv1ADVF354Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    test_case = input('Enter test case description, or enter \\'end\\' to stop: ')\n",
        "    if test_case == 'end':\n",
        "        break\n",
        "    question = input('Ask me something: ')\n",
        "\n",
        "    print(f'{predict_answer(question)}\\n')"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# END OF NOTEBOOK\n",
        "---"
      ],
      "metadata": {
        "id": "Yix-x4lfy4QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2CkAOfd3y6ox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
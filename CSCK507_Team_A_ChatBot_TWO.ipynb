{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_ChatBot_ONE.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_ONE.ipynb",
      "authorship_tag": "ABX9TyN0Y2wGI8DQuhvksf7T+IIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_TWO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSCK507 Natural Language Processing\n",
        "## Team A"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras_preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "########################################################################################################################\n",
        "########################################### DATA PREPARATION ###########################################################\n",
        "########################################################################################################################\n",
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "\n",
        "def get_all_conversations():\n",
        "    all_conversations = []\n",
        "    with codecs.open(\"./cornell movie-dialogs corpus/movie_lines.txt\",\n",
        "                     \"rb\",\n",
        "                     encoding=\"utf-8\",\n",
        "                     errors=\"ignore\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "        for line in lines:\n",
        "            all_conversations.append(line.split(\" +++$+++ \"))\n",
        "    return all_conversations\n",
        "\n",
        "\n",
        "def get_all_sorted_chats(all_conversations):\n",
        "    all_chats = {}\n",
        "    # get only first 10000 conversations from dataset because whole dataset will take 9.16 TiB of RAM\n",
        "### ORIGINAL \n",
        "#for tokens in all_conversations[:10000]:\n",
        "### BEN\n",
        "    for tokens in all_conversations[:2000]:\n",
        "        if len(tokens) > 4:\n",
        "            all_chats[int(tokens[0][1:])] = tokens[4]\n",
        "    return sorted(all_chats.items(), key=lambda x: x[0])\n",
        "\n",
        "\n",
        "def clean_text(text_to_clean):\n",
        "    res = text_to_clean.lower()\n",
        "    res = re.sub(r\"i'm\", \"i am\", res)\n",
        "    res = re.sub(r\"he's\", \"he is\", res)\n",
        "    res = re.sub(r\"she's\", \"she is\", res)\n",
        "    res = re.sub(r\"it's\", \"it is\", res)\n",
        "    res = re.sub(r\"that's\", \"that is\", res)\n",
        "    res = re.sub(r\"what's\", \"what is\", res)\n",
        "    res = re.sub(r\"where's\", \"where is\", res)\n",
        "    res = re.sub(r\"how's\", \"how is\", res)\n",
        "    res = re.sub(r\"\\'ll\", \" will\", res)\n",
        "    res = re.sub(r\"\\'ve\", \" have\", res)\n",
        "    res = re.sub(r\"\\'re\", \" are\", res)\n",
        "    res = re.sub(r\"\\'d\", \" would\", res)\n",
        "    res = re.sub(r\"\\'re\", \" are\", res)\n",
        "    res = re.sub(r\"won't\", \"will not\", res)\n",
        "    res = re.sub(r\"can't\", \"cannot\", res)\n",
        "    res = re.sub(r\"n't\", \" not\", res)\n",
        "    res = re.sub(r\"n'\", \"ng\", res)\n",
        "    res = re.sub(r\"'bout\", \"about\", res)\n",
        "    res = re.sub(r\"'til\", \"until\", res)\n",
        "    res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_conversation_dict(sorted_chats):\n",
        "    conv_dict = {}\n",
        "    counter = 1\n",
        "    conv_ids = []\n",
        "    for i in range(1, len(sorted_chats) + 1):\n",
        "        if i < len(sorted_chats):\n",
        "            if (sorted_chats[i][0] - sorted_chats[i - 1][0]) == 1:\n",
        "                if sorted_chats[i - 1][1] not in conv_ids:\n",
        "                    conv_ids.append(sorted_chats[i - 1][1])\n",
        "                conv_ids.append(sorted_chats[i][1])\n",
        "            elif (sorted_chats[i][0] - sorted_chats[i - 1][0]) > 1:\n",
        "                conv_dict[counter] = conv_ids\n",
        "                conv_ids = []\n",
        "            counter += 1\n",
        "        else:\n",
        "            continue\n",
        "    return conv_dict\n",
        "\n",
        "\n",
        "def get_clean_q_and_a(conversations_dictionary):\n",
        "    ctx_and_target = []\n",
        "    for current_conv in conversations_dictionary.values():\n",
        "        \n",
        "        if len(current_conv) % 2 != 0:\n",
        "            current_conv = current_conv[:-1]\n",
        "        for i in range(0, len(current_conv), 2):\n",
        "            ctx_and_target.append((current_conv[i], current_conv[i + 1]))\n",
        "    context, target = zip(*ctx_and_target)\n",
        "    \n",
        "    context_dirty = list(context)\n",
        "    \n",
        "    clean_questions = list()\n",
        "    for i in range(len(context_dirty)):\n",
        "        clean_questions.append(clean_text(context_dirty[i]))\n",
        "        \n",
        "    target_dirty = list(target)\n",
        "    clean_answers = list()\n",
        "    for i in range(len(target_dirty)):\n",
        "        clean_answers.append('<START> '\n",
        "                             + clean_text(target_dirty[i])\n",
        "                             + ' <END>')\n",
        "    return clean_questions, clean_answers\n",
        "\n",
        "\n",
        "conversations = get_all_conversations()\n",
        "total = len(conversations)\n",
        "print(\"Total conversations in dataset: {}\".format(total))\n",
        "all_sorted_chats = get_all_sorted_chats(conversations)\n",
        "conversation_dictionary = get_conversation_dict(all_sorted_chats)\n",
        "questions, answers = get_clean_q_and_a(conversation_dictionary)\n",
        "print(\"Questions in dataset: {}\".format(len(questions)))\n",
        "print(\"Answers in dataset: {}\".format(len(answers)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "40865d529xqe",
        "outputId": "62aff0d0-7450-45de-8edc-3fbb77a0651c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total conversations in dataset: 304714\n",
            "Questions in dataset: 928\n",
            "Answers in dataset: 928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################################################################################################\n",
        "############################################# MODEL SET-UP #############################################################\n",
        "########################################################################################################################\n",
        "\n",
        "target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
        "### ORIGINAL \n",
        "#tokenizer = Tokenizer(filters=target_regex)\n",
        "### BEN\n",
        "tokenizer = Tokenizer(filters=target_regex, num_words=1500)\n",
        "###\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "### ORIGINAL \n",
        "#VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "### BEN\n",
        "VOCAB_SIZE = 1500\n",
        "\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "encoder_input_data = pad_sequences(tokenized_questions,\n",
        "                                   maxlen=maxlen_questions,\n",
        "                                   padding='post')\n",
        "\n",
        "print(encoder_input_data.shape)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "decoder_input_data = pad_sequences(tokenized_answers,\n",
        "                                   maxlen=maxlen_answers,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data.shape)\n",
        "\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "fe4a5e56-5873-4afa-95b8-98b6743c49fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size : 1500\n",
            "(928, 98)\n",
            "(928, 127)\n",
            "(928, 127, 1500)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 200)    300000      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 200)    300000      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 200),        320800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 200),  320800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 1500)   301500      ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,543,100\n",
            "Trainable params: 1,543,100\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "############################################# MODEL TRAINING ###########################################################\n",
        "########################################################################################################################\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],\n",
        "          decoder_output_data,\n",
        "          batch_size=50,\n",
        "          epochs=300)\n",
        "model.save('/content/drive/MyDrive/CSCK507_Team_A/model_big.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "d3052ded-1689-4bc8-b921-0d81789cdc1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "19/19 [==============================] - 10s 497ms/step - loss: 0.4155\n",
            "Epoch 2/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.4033\n",
            "Epoch 3/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.3960\n",
            "Epoch 4/300\n",
            "19/19 [==============================] - 9s 484ms/step - loss: 0.3906\n",
            "Epoch 5/300\n",
            "19/19 [==============================] - 9s 488ms/step - loss: 0.3860\n",
            "Epoch 6/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.3816\n",
            "Epoch 7/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.3773\n",
            "Epoch 8/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.3731\n",
            "Epoch 9/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.3692\n",
            "Epoch 10/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.3651\n",
            "Epoch 11/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.3611\n",
            "Epoch 12/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.3573\n",
            "Epoch 13/300\n",
            "19/19 [==============================] - 10s 503ms/step - loss: 0.3535\n",
            "Epoch 14/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.3497\n",
            "Epoch 15/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.3459\n",
            "Epoch 16/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.3418\n",
            "Epoch 17/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.3380\n",
            "Epoch 18/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.3339\n",
            "Epoch 19/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.3299\n",
            "Epoch 20/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.3259\n",
            "Epoch 21/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.3221\n",
            "Epoch 22/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.3181\n",
            "Epoch 23/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.3144\n",
            "Epoch 24/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.3103\n",
            "Epoch 25/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.3067\n",
            "Epoch 26/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.3027\n",
            "Epoch 27/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.2992\n",
            "Epoch 28/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.2950\n",
            "Epoch 29/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.2917\n",
            "Epoch 30/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.2879\n",
            "Epoch 31/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.2843\n",
            "Epoch 32/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.2808\n",
            "Epoch 33/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.2768\n",
            "Epoch 34/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.2733\n",
            "Epoch 35/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.2696\n",
            "Epoch 36/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.2662\n",
            "Epoch 37/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.2621\n",
            "Epoch 38/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.2586\n",
            "Epoch 39/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.2549\n",
            "Epoch 40/300\n",
            "19/19 [==============================] - 10s 507ms/step - loss: 0.2513\n",
            "Epoch 41/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.2474\n",
            "Epoch 42/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.2437\n",
            "Epoch 43/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.2398\n",
            "Epoch 44/300\n",
            "19/19 [==============================] - 9s 488ms/step - loss: 0.2361\n",
            "Epoch 45/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.2320\n",
            "Epoch 46/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.2281\n",
            "Epoch 47/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.2244\n",
            "Epoch 48/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.2207\n",
            "Epoch 49/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.2166\n",
            "Epoch 50/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.2125\n",
            "Epoch 51/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.2087\n",
            "Epoch 52/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.2052\n",
            "Epoch 53/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.2008\n",
            "Epoch 54/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1969\n",
            "Epoch 55/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.1932\n",
            "Epoch 56/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1894\n",
            "Epoch 57/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.1857\n",
            "Epoch 58/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.1816\n",
            "Epoch 59/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.1783\n",
            "Epoch 60/300\n",
            "19/19 [==============================] - 9s 500ms/step - loss: 0.1742\n",
            "Epoch 61/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.1701\n",
            "Epoch 62/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.1671\n",
            "Epoch 63/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.1624\n",
            "Epoch 64/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.1596\n",
            "Epoch 65/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.1558\n",
            "Epoch 66/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1518\n",
            "Epoch 67/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1485\n",
            "Epoch 68/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.1447\n",
            "Epoch 69/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1413\n",
            "Epoch 70/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1373\n",
            "Epoch 71/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.1344\n",
            "Epoch 72/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1309\n",
            "Epoch 73/300\n",
            "19/19 [==============================] - 10s 512ms/step - loss: 0.1274\n",
            "Epoch 74/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.1243\n",
            "Epoch 75/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1206\n",
            "Epoch 76/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1177\n",
            "Epoch 77/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.1144\n",
            "Epoch 78/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.1113\n",
            "Epoch 79/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1077\n",
            "Epoch 80/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.1055\n",
            "Epoch 81/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.1021\n",
            "Epoch 82/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0995\n",
            "Epoch 83/300\n",
            "19/19 [==============================] - 9s 500ms/step - loss: 0.0960\n",
            "Epoch 84/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0938\n",
            "Epoch 85/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 0.0902\n",
            "Epoch 86/300\n",
            "19/19 [==============================] - 10s 512ms/step - loss: 0.0875\n",
            "Epoch 87/300\n",
            "19/19 [==============================] - 10s 516ms/step - loss: 0.0854\n",
            "Epoch 88/300\n",
            "19/19 [==============================] - 10s 511ms/step - loss: 0.0829\n",
            "Epoch 89/300\n",
            "19/19 [==============================] - 10s 514ms/step - loss: 0.0801\n",
            "Epoch 90/300\n",
            "19/19 [==============================] - 10s 513ms/step - loss: 0.0776\n",
            "Epoch 91/300\n",
            "19/19 [==============================] - 10s 509ms/step - loss: 0.0754\n",
            "Epoch 92/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0723\n",
            "Epoch 93/300\n",
            "19/19 [==============================] - 10s 507ms/step - loss: 0.0703\n",
            "Epoch 94/300\n",
            "19/19 [==============================] - 10s 505ms/step - loss: 0.0679\n",
            "Epoch 95/300\n",
            "19/19 [==============================] - 10s 507ms/step - loss: 0.0657\n",
            "Epoch 96/300\n",
            "19/19 [==============================] - 10s 508ms/step - loss: 0.0635\n",
            "Epoch 97/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0609\n",
            "Epoch 98/300\n",
            "19/19 [==============================] - 10s 509ms/step - loss: 0.0593\n",
            "Epoch 99/300\n",
            "19/19 [==============================] - 10s 505ms/step - loss: 0.0572\n",
            "Epoch 100/300\n",
            "19/19 [==============================] - 10s 506ms/step - loss: 0.0548\n",
            "Epoch 101/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0531\n",
            "Epoch 102/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 0.0513\n",
            "Epoch 103/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0493\n",
            "Epoch 104/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0474\n",
            "Epoch 105/300\n",
            "19/19 [==============================] - 10s 513ms/step - loss: 0.0453\n",
            "Epoch 106/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0439\n",
            "Epoch 107/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0421\n",
            "Epoch 108/300\n",
            "19/19 [==============================] - 10s 513ms/step - loss: 0.0405\n",
            "Epoch 109/300\n",
            "19/19 [==============================] - 10s 510ms/step - loss: 0.0393\n",
            "Epoch 110/300\n",
            "19/19 [==============================] - 10s 513ms/step - loss: 0.0378\n",
            "Epoch 111/300\n",
            "19/19 [==============================] - 10s 506ms/step - loss: 0.0361\n",
            "Epoch 112/300\n",
            "19/19 [==============================] - 10s 510ms/step - loss: 0.0347\n",
            "Epoch 113/300\n",
            "19/19 [==============================] - 10s 513ms/step - loss: 0.0327\n",
            "Epoch 114/300\n",
            "19/19 [==============================] - 10s 512ms/step - loss: 0.0316\n",
            "Epoch 115/300\n",
            "19/19 [==============================] - 10s 508ms/step - loss: 0.0313\n",
            "Epoch 116/300\n",
            "19/19 [==============================] - 10s 509ms/step - loss: 0.0295\n",
            "Epoch 117/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0276\n",
            "Epoch 118/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0270\n",
            "Epoch 119/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0257\n",
            "Epoch 120/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0247\n",
            "Epoch 121/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0232\n",
            "Epoch 122/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0224\n",
            "Epoch 123/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0214\n",
            "Epoch 124/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0206\n",
            "Epoch 125/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0196\n",
            "Epoch 126/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0192\n",
            "Epoch 127/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0178\n",
            "Epoch 128/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0171\n",
            "Epoch 129/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0162\n",
            "Epoch 130/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0153\n",
            "Epoch 131/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0151\n",
            "Epoch 132/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.0140\n",
            "Epoch 133/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0132\n",
            "Epoch 134/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0128\n",
            "Epoch 135/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0121\n",
            "Epoch 136/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0114\n",
            "Epoch 137/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0110\n",
            "Epoch 138/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0104\n",
            "Epoch 139/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0099\n",
            "Epoch 140/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0094\n",
            "Epoch 141/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0090\n",
            "Epoch 142/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0085\n",
            "Epoch 143/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0082\n",
            "Epoch 144/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0078\n",
            "Epoch 145/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0072\n",
            "Epoch 146/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.0069\n",
            "Epoch 147/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0066\n",
            "Epoch 148/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0066\n",
            "Epoch 149/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.0059\n",
            "Epoch 150/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0056\n",
            "Epoch 151/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0057\n",
            "Epoch 152/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0053\n",
            "Epoch 153/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0047\n",
            "Epoch 154/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0048\n",
            "Epoch 155/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0046\n",
            "Epoch 156/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0042\n",
            "Epoch 157/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0041\n",
            "Epoch 158/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0041\n",
            "Epoch 159/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0038\n",
            "Epoch 160/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0034\n",
            "Epoch 161/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0033\n",
            "Epoch 162/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0033\n",
            "Epoch 163/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0032\n",
            "Epoch 164/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0030\n",
            "Epoch 165/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0028\n",
            "Epoch 166/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 0.0029\n",
            "Epoch 167/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0027\n",
            "Epoch 168/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0025\n",
            "Epoch 169/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0025\n",
            "Epoch 170/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.0025\n",
            "Epoch 171/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0024\n",
            "Epoch 172/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0022\n",
            "Epoch 173/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0021\n",
            "Epoch 174/300\n",
            "19/19 [==============================] - 10s 504ms/step - loss: 0.0020\n",
            "Epoch 175/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0021\n",
            "Epoch 176/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0020\n",
            "Epoch 177/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0019\n",
            "Epoch 178/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0019\n",
            "Epoch 179/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0021\n",
            "Epoch 180/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0021\n",
            "Epoch 181/300\n",
            "19/19 [==============================] - 10s 510ms/step - loss: 0.0016\n",
            "Epoch 182/300\n",
            "19/19 [==============================] - 10s 549ms/step - loss: 0.0016\n",
            "Epoch 183/300\n",
            "19/19 [==============================] - 10s 531ms/step - loss: 0.0016\n",
            "Epoch 184/300\n",
            "19/19 [==============================] - 10s 503ms/step - loss: 0.0016\n",
            "Epoch 185/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 0.0017\n",
            "Epoch 186/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0015\n",
            "Epoch 187/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0014\n",
            "Epoch 188/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0015\n",
            "Epoch 189/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0014\n",
            "Epoch 190/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0014\n",
            "Epoch 191/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0014\n",
            "Epoch 192/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0014\n",
            "Epoch 193/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0014\n",
            "Epoch 194/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0013\n",
            "Epoch 195/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0013\n",
            "Epoch 196/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0013\n",
            "Epoch 197/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.0013\n",
            "Epoch 198/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0013\n",
            "Epoch 199/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0012\n",
            "Epoch 200/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0011\n",
            "Epoch 201/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0012\n",
            "Epoch 202/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0011\n",
            "Epoch 203/300\n",
            "19/19 [==============================] - 10s 503ms/step - loss: 0.0013\n",
            "Epoch 204/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0011\n",
            "Epoch 205/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0011\n",
            "Epoch 206/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 0.0012\n",
            "Epoch 207/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.0012\n",
            "Epoch 208/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0012\n",
            "Epoch 209/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0011\n",
            "Epoch 210/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0011\n",
            "Epoch 211/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0012\n",
            "Epoch 212/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0012\n",
            "Epoch 213/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 9.9708e-04\n",
            "Epoch 214/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0010\n",
            "Epoch 215/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0010\n",
            "Epoch 216/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0010\n",
            "Epoch 217/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.0013\n",
            "Epoch 218/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0012\n",
            "Epoch 219/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 9.8896e-04\n",
            "Epoch 220/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.7610e-04\n",
            "Epoch 221/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 9.7559e-04\n",
            "Epoch 222/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0010\n",
            "Epoch 223/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 9.9741e-04\n",
            "Epoch 224/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 9.5913e-04\n",
            "Epoch 225/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 9.5744e-04\n",
            "Epoch 226/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 9.3015e-04\n",
            "Epoch 227/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.6593e-04\n",
            "Epoch 228/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.0011\n",
            "Epoch 229/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0010\n",
            "Epoch 230/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0010\n",
            "Epoch 231/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.9819e-04\n",
            "Epoch 232/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 9.1686e-04\n",
            "Epoch 233/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.0205e-04\n",
            "Epoch 234/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 9.2152e-04\n",
            "Epoch 235/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 9.1654e-04\n",
            "Epoch 236/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 9.1228e-04\n",
            "Epoch 237/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 9.6582e-04\n",
            "Epoch 238/300\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0010\n",
            "Epoch 239/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0010\n",
            "Epoch 240/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 8.9716e-04\n",
            "Epoch 241/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 8.7299e-04\n",
            "Epoch 242/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.8309e-04\n",
            "Epoch 243/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 9.0480e-04\n",
            "Epoch 244/300\n",
            "19/19 [==============================] - 9s 491ms/step - loss: 8.6893e-04\n",
            "Epoch 245/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 9.0318e-04\n",
            "Epoch 246/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 8.6770e-04\n",
            "Epoch 247/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.7261e-04\n",
            "Epoch 248/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.7311e-04\n",
            "Epoch 249/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 8.7945e-04\n",
            "Epoch 250/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.6470e-04\n",
            "Epoch 251/300\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 8.8498e-04\n",
            "Epoch 252/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.5639e-04\n",
            "Epoch 253/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.5836e-04\n",
            "Epoch 254/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.8065e-04\n",
            "Epoch 255/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.6559e-04\n",
            "Epoch 256/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.4116e-04\n",
            "Epoch 257/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 8.4033e-04\n",
            "Epoch 258/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.6170e-04\n",
            "Epoch 259/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.5124e-04\n",
            "Epoch 260/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.4755e-04\n",
            "Epoch 261/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.5316e-04\n",
            "Epoch 262/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.4189e-04\n",
            "Epoch 263/300\n",
            "19/19 [==============================] - 9s 500ms/step - loss: 8.4060e-04\n",
            "Epoch 264/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.3757e-04\n",
            "Epoch 265/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 8.2568e-04\n",
            "Epoch 266/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.2815e-04\n",
            "Epoch 267/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 8.1901e-04\n",
            "Epoch 268/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 8.3054e-04\n",
            "Epoch 269/300\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 8.4876e-04\n",
            "Epoch 270/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.2303e-04\n",
            "Epoch 271/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 8.2851e-04\n",
            "Epoch 272/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.1645e-04\n",
            "Epoch 273/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.1634e-04\n",
            "Epoch 274/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.2717e-04\n",
            "Epoch 275/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0011\n",
            "Epoch 276/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 9.4826e-04\n",
            "Epoch 277/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.9365e-04\n",
            "Epoch 278/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 9.9710e-04\n",
            "Epoch 279/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.3490e-04\n",
            "Epoch 280/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0010\n",
            "Epoch 281/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 7.9856e-04\n",
            "Epoch 282/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.0993e-04\n",
            "Epoch 283/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 9.4357e-04\n",
            "Epoch 284/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 0.0011\n",
            "Epoch 285/300\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0011\n",
            "Epoch 286/300\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 8.6276e-04\n",
            "Epoch 287/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.1569e-04\n",
            "Epoch 288/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 8.0135e-04\n",
            "Epoch 289/300\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 8.0010e-04\n",
            "Epoch 290/300\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 8.0818e-04\n",
            "Epoch 291/300\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 8.1284e-04\n",
            "Epoch 292/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.0359e-04\n",
            "Epoch 293/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 8.0590e-04\n",
            "Epoch 294/300\n",
            "19/19 [==============================] - 9s 496ms/step - loss: 8.1506e-04\n",
            "Epoch 295/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.1381e-04\n",
            "Epoch 296/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 7.9163e-04\n",
            "Epoch 297/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.1698e-04\n",
            "Epoch 298/300\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 7.9511e-04\n",
            "Epoch 299/300\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 8.0030e-04\n",
            "Epoch 300/300\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 8.0035e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/model_big.h5')\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(2):\n",
        "    states_values = enc_model.predict(\n",
        "        str_to_tokens(input('Enter question : ')))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'end':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "fdf53185-9073-4503-a533-659888130949"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 200)    300000      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 200),  320800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'input_7[0][0]',                \n",
            "                                 (None, 200)]                     'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 1500)   301500      ['lstm_3[2][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 922,300\n",
            "Trainable params: 922,300\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 200)         300000    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 620,800\n",
            "Trainable params: 620,800\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Enter question : how do you get your hair to look like that\n",
            " s every two days and i never ever use a without the\n",
            "Enter question : how you get hair look like that\n",
            " fuck thousand i do not know what i will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[800]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QObKQwyVLNzY",
        "outputId": "91277479-4922-4790-fc47-76b3d533b395"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START> well the crew is confined to the ship when we land at clavius we have to stay inside for the time it take to refit  about twentyfour hours and then we are going to back empty <END>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "outputId": "f20dc915-95bc-4ba3-fb80-d78093953908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[100]"
      ],
      "metadata": {
        "id": "witWNiXl5Gwh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6caef289-2083-499a-b2e9-9b2643b273cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how do you get your hair to look like that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tBNrhdhyKJiu",
        "outputId": "b4142414-36ba-42d6-d8e8-91989c249f01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> eber's deep conditioner every two days and i never ever use a blowdryer without the diffuser attachment <END>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UqK-qDFOKKqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
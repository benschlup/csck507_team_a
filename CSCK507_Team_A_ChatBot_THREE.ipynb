{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_ChatBot_ONE.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyNZ8iH17gZTQBhw5OnmPQHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSCK507 Natural Language Processing\n",
        "## Team A"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "urllib.request.urlretrieve(\"https://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\", \"Question_Answer_Dataset_v1.2.tar.gz\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "563a6e8c-a1bd-4b0c-c383-5ba09a81eade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Question_Answer_Dataset_v1.2.tar.gz',\n",
              " <http.client.HTTPMessage at 0x7fba7a533150>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "file = tarfile.open('Question_Answer_Dataset_v1.2.tar.gz')\n",
        "file.extractall('.')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers from all courses in Spring 2008, 2009 and 2010 respectively\n",
        "qa_df = pd.DataFrame()\n",
        "for course in ['S08', 'S09', 'S10']:\n",
        "    print(f'Reading questions and answers from course {course}')\n",
        "    course_qa_df = pd.read_csv( f'./Question_Answer_Dataset_v1.2/{course}/question_answer_pairs.txt', sep='\\t', encoding='ISO-8859-1')\n",
        "    course_qa_df['course'] = course\n",
        "    qa_df = pd.concat([qa_df, course_qa_df])\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK",
        "outputId": "abf1c7ae-0cab-4459-ca57-66fccf99125f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading questions and answers from course S08\n",
            "Reading questions and answers from course S09\n",
            "Reading questions and answers from course S10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove lines not having answers (or not even having questions, in some cases...):\n",
        "qa_df = qa_df[qa_df['Answer'].notna()]"
      ],
      "metadata": {
        "id": "RY7VF6eGNUbj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove duplicates\n",
        "# Add the length of the answer to the dataframe\n",
        "qa_df['answer_length'] = qa_df['Answer'].str.len()\n",
        "# Sort the dataframe to have the longest answer per question at the top\n",
        "qa_df.sort_values(['Question', 'answer_length'], inplace=True)\n",
        "# Remove duplicated questions, retaining only the longest answer\n",
        "qa_df.drop_duplicates(subset=['Question'], keep='last', inplace=True)\n"
      ],
      "metadata": {
        "id": "z8aXSVNSP-zh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Questions are easy to derive\n",
        "#questions = qa_df['Question']\n",
        "questions = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower() for s in qa_df['Question'] ]\n",
        "\n",
        "# Answers are no harder\n",
        "answers = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower()+' _STOP_' for s in qa_df['Answer'] ]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################################################################################################\n",
        "############################################# MODEL SET-UP #############################################################\n",
        "########################################################################################################################\n",
        "\n",
        "target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
        "### ORIGINAL \n",
        "tokenizer = Tokenizer(filters=target_regex)\n",
        "### BEN\n",
        "#tokenizer = Tokenizer(filters=target_regex, num_words=1500)\n",
        "###\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "### ORIGINAL \n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "### BEN\n",
        "#VOCAB_SIZE = 1500\n",
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "encoder_input_data = pad_sequences(tokenized_questions,\n",
        "                                   maxlen=maxlen_questions,\n",
        "                                   padding='post')\n",
        "\n",
        "print(encoder_input_data.shape)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "decoder_input_data = pad_sequences(tokenized_answers,\n",
        "                                   maxlen=maxlen_answers,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data.shape)\n",
        "\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "50df186a-72fb-4b93-81a7-54d9783ae68c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2203, 44)\n",
            "(2203, 157)\n",
            "(2203, 157, 5240)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 200)    1048000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    1048000     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 5240)   1053240     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,790,840\n",
            "Trainable params: 3,790,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "############################################# MODEL TRAINING ###########################################################\n",
        "########################################################################################################################\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],\n",
        "          decoder_output_data,\n",
        "          batch_size=50,\n",
        "          epochs=100)\n",
        "model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "c274e8b8-1c49-45bc-bb9c-fff9421cb929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "45/45 [==============================] - 11s 67ms/step - loss: 0.2444\n",
            "Epoch 2/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.2032\n",
            "Epoch 3/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.1912\n",
            "Epoch 4/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.1847\n",
            "Epoch 5/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.1799\n",
            "Epoch 6/100\n",
            "45/45 [==============================] - 3s 67ms/step - loss: 0.1748\n",
            "Epoch 7/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.1695\n",
            "Epoch 8/100\n",
            "45/45 [==============================] - 3s 65ms/step - loss: 0.1652\n",
            "Epoch 9/100\n",
            "45/45 [==============================] - 3s 67ms/step - loss: 0.1613\n",
            "Epoch 10/100\n",
            "45/45 [==============================] - 3s 65ms/step - loss: 0.1580\n",
            "Epoch 11/100\n",
            "45/45 [==============================] - 3s 65ms/step - loss: 0.1542\n",
            "Epoch 12/100\n",
            "45/45 [==============================] - 3s 65ms/step - loss: 0.1505\n",
            "Epoch 13/100\n",
            "45/45 [==============================] - 3s 65ms/step - loss: 0.1467\n",
            "Epoch 14/100\n",
            "45/45 [==============================] - 3s 66ms/step - loss: 0.1436\n",
            "Epoch 15/100\n",
            "39/45 [=========================>....] - ETA: 0s - loss: 0.1393"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n"
      ],
      "metadata": {
        "id": "iHYIs3pL86Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input('Ask me something, or enter \\'end\\' to stop: ')\n",
        "    if question == 'end':\n",
        "        break\n",
        "    states_values = enc_model.predict(str_to_tokens(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get some random numbers to choose random sentences\n",
        "rand_integers = [random.randint(0, len(questions)-1) for i in range(1, 100)]\n",
        "bleu_total = 0\n",
        "\n",
        "for i in rand_integers:\n",
        "    states_values = enc_model.predict(str_to_tokens(questions[i]))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    decoded_translation = decoded_translation[1:]\n",
        "\n",
        "    print(f'Original question: {questions[i]}')\n",
        "    print(f'Predicated answer: {decoded_translation}')\n",
        "    print(f'Actual answer: {answers[i][8:-7]}')\n",
        "    \n",
        "    # The following should contain all possible answers, though...\n",
        "    reference_answers = [answers[i][8:-7]]\n",
        "    print(f'{reference_answers}')\n",
        "    bleu_score = sentence_bleu(reference_answers, decoded_translation, smoothing_function=SmoothingFunction().method0)\n",
        "    print(f'Bleu score: {bleu_score}\\n')\n",
        "    bleu_total += bleu_score\n",
        "\n",
        "print(f'Bleu average = {bleu_total/len(rand_integers)}')\n",
        "    "
      ],
      "metadata": {
        "id": "QObKQwyVLNzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(questions)"
      ],
      "metadata": {
        "id": "OwygnS0UfrP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers[100]"
      ],
      "metadata": {
        "id": "tBNrhdhyKJiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UqK-qDFOKKqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}